{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## Специализация \"Машинное обучение и анализ данных\"\n",
    "<center>Автор материала: программист-исследователь Mail.Ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ [Юрий Кашницкий](https://yorko.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1 <br> Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Catch Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "1. Заполните код в этой тетрадке \n",
    "2. Если вы проходите специализацию Яндеса и МФТИ, пошлите тетрадку в соответствующем Peer Review. <br> Если вы проходите курс ODS, выберите ответы в [веб-форме](https://docs.google.com/forms/d/1pLsegkAICL9PzOLyAeH9DmDOBfktte0l8JW75uWcTng). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = 'capstone_user_identification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-03-28 10:52:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:52:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:53:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:54:12</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-28 10:54:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:55:42</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:12</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-03-28 10:56:42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>2014-02-28 10:53:05</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:22</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:23</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>175.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:55:59</td>\n",
       "      <td>177.0</td>\n",
       "      <td>2014-02-28 10:57:06</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2014-02-28 10:57:11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1  site2                time2    site3  \\\n",
       "session_id                                                                    \n",
       "1             718  2014-02-20 10:02:45    NaN                  NaN      NaN   \n",
       "2             890  2014-02-22 11:19:50  941.0  2014-02-22 11:19:50   3847.0   \n",
       "3           14769  2013-12-16 16:40:17   39.0  2013-12-16 16:40:18  14768.0   \n",
       "4             782  2014-03-28 10:52:12  782.0  2014-03-28 10:52:42    782.0   \n",
       "5              22  2014-02-28 10:53:05  177.0  2014-02-28 10:55:22    175.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1                           NaN      NaN                  NaN    NaN   \n",
       "2           2014-02-22 11:19:51    941.0  2014-02-22 11:19:51  942.0   \n",
       "3           2013-12-16 16:40:19  14769.0  2013-12-16 16:40:19   37.0   \n",
       "4           2014-03-28 10:53:12    782.0  2014-03-28 10:53:42  782.0   \n",
       "5           2014-02-28 10:55:22    178.0  2014-02-28 10:55:23  177.0   \n",
       "\n",
       "                          time5  ...                time6    site7  \\\n",
       "session_id                       ...                                 \n",
       "1                           NaN  ...                  NaN      NaN   \n",
       "2           2014-02-22 11:19:51  ...  2014-02-22 11:19:51   3847.0   \n",
       "3           2013-12-16 16:40:19  ...  2013-12-16 16:40:19  14768.0   \n",
       "4           2014-03-28 10:54:12  ...  2014-03-28 10:54:42    782.0   \n",
       "5           2014-02-28 10:55:23  ...  2014-02-28 10:55:59    175.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1                           NaN      NaN                  NaN      NaN   \n",
       "2           2014-02-22 11:19:52   3846.0  2014-02-22 11:19:52   1516.0   \n",
       "3           2013-12-16 16:40:20  14768.0  2013-12-16 16:40:21  14768.0   \n",
       "4           2014-03-28 10:55:12    782.0  2014-03-28 10:55:42    782.0   \n",
       "5           2014-02-28 10:55:59    177.0  2014-02-28 10:55:59    177.0   \n",
       "\n",
       "                          time9   site10               time10 target  \n",
       "session_id                                                            \n",
       "1                           NaN      NaN                  NaN      0  \n",
       "2           2014-02-22 11:20:15   1518.0  2014-02-22 11:20:16      0  \n",
       "3           2013-12-16 16:40:22  14768.0  2013-12-16 16:40:24      0  \n",
       "4           2014-03-28 10:56:12    782.0  2014-03-28 10:56:42      0  \n",
       "5           2014-02-28 10:57:06    178.0  2014-02-28 10:57:11      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 21), (82797, 20))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336358, 21)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 253561 entries, 1 to 253561\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   site1   253561 non-null  int64  \n",
      " 1   time1   253561 non-null  object \n",
      " 2   site2   250098 non-null  float64\n",
      " 3   time2   250098 non-null  object \n",
      " 4   site3   246919 non-null  float64\n",
      " 5   time3   246919 non-null  object \n",
      " 6   site4   244321 non-null  float64\n",
      " 7   time4   244321 non-null  object \n",
      " 8   site5   241829 non-null  float64\n",
      " 9   time5   241829 non-null  object \n",
      " 10  site6   239495 non-null  float64\n",
      " 11  time6   239495 non-null  object \n",
      " 12  site7   237297 non-null  float64\n",
      " 13  time7   237297 non-null  object \n",
      " 14  site8   235224 non-null  float64\n",
      " 15  time8   235224 non-null  object \n",
      " 16  site9   233084 non-null  float64\n",
      " 17  time9   233084 non-null  object \n",
      " 18  site10  231052 non-null  float64\n",
      " 19  time10  231052 non-null  object \n",
      " 20  target  253561 non-null  int64  \n",
      "dtypes: float64(9), int64(2), object(10)\n",
      "memory usage: 42.6+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>44582.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "      <td>15336.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>782</td>\n",
       "      <td>2014-07-03 11:00:28</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:53</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:58</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:06</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:09</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:10</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:23</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:29</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:30</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>2014-12-05 15:55:12</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:13</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:14</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:15</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:17</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:18</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:19</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:33</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1023</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>2014-11-04 10:03:19</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:20</td>\n",
       "      <td>222.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>202.0</td>\n",
       "      <td>2014-11-04 10:03:21</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:22</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "      <td>3374.0</td>\n",
       "      <td>2014-11-04 10:03:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>301</td>\n",
       "      <td>2014-05-16 15:05:31</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:32</td>\n",
       "      <td>301.0</td>\n",
       "      <td>2014-05-16 15:05:33</td>\n",
       "      <td>66.0</td>\n",
       "      <td>2014-05-16 15:05:39</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>70.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2014-05-16 15:05:40</td>\n",
       "      <td>167.0</td>\n",
       "      <td>2014-05-16 15:05:44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2  site3  \\\n",
       "session_id                                                                   \n",
       "1              29  2014-10-04 11:19:53    35.0  2014-10-04 11:19:53   22.0   \n",
       "2             782  2014-07-03 11:00:28   782.0  2014-07-03 11:00:53  782.0   \n",
       "3              55  2014-12-05 15:55:12    55.0  2014-12-05 15:55:13   55.0   \n",
       "4            1023  2014-11-04 10:03:19  1022.0  2014-11-04 10:03:19   50.0   \n",
       "5             301  2014-05-16 15:05:31   301.0  2014-05-16 15:05:32  301.0   \n",
       "\n",
       "                          time3  site4                time4  site5  \\\n",
       "session_id                                                           \n",
       "1           2014-10-04 11:19:54  321.0  2014-10-04 11:19:54   23.0   \n",
       "2           2014-07-03 11:00:58  782.0  2014-07-03 11:01:06  782.0   \n",
       "3           2014-12-05 15:55:14   55.0  2014-12-05 15:56:15   55.0   \n",
       "4           2014-11-04 10:03:20  222.0  2014-11-04 10:03:21  202.0   \n",
       "5           2014-05-16 15:05:33   66.0  2014-05-16 15:05:39   67.0   \n",
       "\n",
       "                          time5   site6                time6   site7  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54  2211.0  2014-10-04 11:19:54  6730.0   \n",
       "2           2014-07-03 11:01:09   782.0  2014-07-03 11:01:10   782.0   \n",
       "3           2014-12-05 15:56:16    55.0  2014-12-05 15:56:17    55.0   \n",
       "4           2014-11-04 10:03:21  3374.0  2014-11-04 10:03:22    50.0   \n",
       "5           2014-05-16 15:05:40    69.0  2014-05-16 15:05:40    70.0   \n",
       "\n",
       "                          time7  site8                time8    site9  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 11:19:54   21.0  2014-10-04 11:19:54  44582.0   \n",
       "2           2014-07-03 11:01:23  782.0  2014-07-03 11:01:29    782.0   \n",
       "3           2014-12-05 15:56:18   55.0  2014-12-05 15:56:19   1445.0   \n",
       "4           2014-11-04 10:03:22   48.0  2014-11-04 10:03:22     48.0   \n",
       "5           2014-05-16 15:05:40   68.0  2014-05-16 15:05:40     71.0   \n",
       "\n",
       "                          time9   site10               time10  \n",
       "session_id                                                     \n",
       "1           2014-10-04 11:20:00  15336.0  2014-10-04 11:20:00  \n",
       "2           2014-07-03 11:01:30    782.0  2014-07-03 11:01:53  \n",
       "3           2014-12-05 15:56:33   1445.0  2014-12-05 15:56:36  \n",
       "4           2014-11-04 10:03:23   3374.0  2014-11-04 10:03:23  \n",
       "5           2014-05-16 15:05:40    167.0  2014-05-16 15:05:44  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 82797 entries, 1 to 82797\n",
      "Data columns (total 20 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   site1   82797 non-null  int64  \n",
      " 1   time1   82797 non-null  object \n",
      " 2   site2   81308 non-null  float64\n",
      " 3   time2   81308 non-null  object \n",
      " 4   site3   80075 non-null  float64\n",
      " 5   time3   80075 non-null  object \n",
      " 6   site4   79182 non-null  float64\n",
      " 7   time4   79182 non-null  object \n",
      " 8   site5   78341 non-null  float64\n",
      " 9   time5   78341 non-null  object \n",
      " 10  site6   77566 non-null  float64\n",
      " 11  time6   77566 non-null  object \n",
      " 12  site7   76840 non-null  float64\n",
      " 13  time7   76840 non-null  object \n",
      " 14  site8   76151 non-null  float64\n",
      " 15  time8   76151 non-null  object \n",
      " 16  site9   75484 non-null  float64\n",
      " 17  time9   75484 non-null  object \n",
      " 18  site10  74806 non-null  float64\n",
      " 19  time10  74806 non-null  object \n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 13.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 2297 сессий одного пользователя (Alice) и 251264 сессий – других пользователей, не Элис. Дисбаланс классов очень сильный, и смотреть на долю верных ответов (accuracy) непоказательно.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    251264\n",
       "1      2297\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[[\n",
    "    'site%d' % i for i in range(1, 11)]].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>718</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "      <td>941</td>\n",
       "      <td>3847</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>3846</td>\n",
       "      <td>3847</td>\n",
       "      <td>3846</td>\n",
       "      <td>1516</td>\n",
       "      <td>1518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14769</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14769</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "      <td>14768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "      <td>782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22</td>\n",
       "      <td>177</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>803</td>\n",
       "      <td>23</td>\n",
       "      <td>5956</td>\n",
       "      <td>17513</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>803</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "      <td>17514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>5041</td>\n",
       "      <td>14422</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>5041</td>\n",
       "      <td>14421</td>\n",
       "      <td>14421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>668</td>\n",
       "      <td>940</td>\n",
       "      <td>942</td>\n",
       "      <td>941</td>\n",
       "      <td>941</td>\n",
       "      <td>942</td>\n",
       "      <td>940</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3700</td>\n",
       "      <td>229</td>\n",
       "      <td>570</td>\n",
       "      <td>21</td>\n",
       "      <td>229</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>2336</td>\n",
       "      <td>2044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1             718      0      0      0      0      0      0      0      0   \n",
       "2             890    941   3847    941    942   3846   3847   3846   1516   \n",
       "3           14769     39  14768  14769     37     39  14768  14768  14768   \n",
       "4             782    782    782    782    782    782    782    782    782   \n",
       "5              22    177    175    178    177    178    175    177    177   \n",
       "6             570     21    570     21     21      0      0      0      0   \n",
       "7             803     23   5956  17513     37     21    803  17514  17514   \n",
       "8              22     21     29   5041  14422     23     21   5041  14421   \n",
       "9             668    940    942    941    941    942    940     23     21   \n",
       "10           3700    229    570     21    229     21     21     21   2336   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1                0  \n",
       "2             1518  \n",
       "3            14768  \n",
       "4              782  \n",
       "5              178  \n",
       "6                0  \n",
       "7            17514  \n",
       "8            14421  \n",
       "9               22  \n",
       "10            2044  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу *train_test_df_sites*, потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "Обратите внимание на то, что в  сессиях меньше 10 сайтов  у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_matrix(docs):\n",
    "    \"\"\"\n",
    "    Returns parameters for construction sparse matrix from initial `docs` table.\n",
    "    vocabulary - dictionary of terms, value as index.\n",
    "    indicies - list of term indicies.\n",
    "    data - list of data existence\n",
    "    indptr - \n",
    "    \"\"\"\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for d in docs:\n",
    "        for term in d:\n",
    "            index = vocabulary.setdefault(term, term)\n",
    "            indices.append(index)\n",
    "            data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return data, indices, indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse(X, nr=(1, 1), tp='\\w+'):\n",
    "    # Determine vocabulary_ of Counting ngrams\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=nr, token_pattern=tp)\n",
    "    vectorizer.fit_transform(\n",
    "        [\" \".join(map(str, row)) for row in X])\n",
    "    voc = vectorizer.vocabulary_\n",
    "    # Remove '0's from vocabulary_\n",
    "    remove = [k for k in voc.keys() if (\n",
    "        len(re.findall(r'(^0$)|( 0 )|(^0 )', k)))]\n",
    "    for k in remove:\n",
    "        del voc[k]\n",
    "    # Reindexing\n",
    "    ind = 0\n",
    "    for k in voc:\n",
    "        voc[k] = ind\n",
    "        ind += 1\n",
    "    # Transform again with new vocabulary\n",
    "    vectorizer = CountVectorizer(\n",
    "        ngram_range=nr, token_pattern=tp, vocabulary=voc)\n",
    "    return (vectorizer.fit_transform(\n",
    "        [\" \".join(map(str, row)) for row in X]), vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sparse = csr_matrix(term_matrix(\n",
    "    np.array(train_test_df_sites)))[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336358, 48371)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or through CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = get_sparse(train_test_df_sites.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(336358, 48371)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_sparse.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размер неразряженной матрицы 121 Gb (если размер одной ячейки 64bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "train_test_sparse.shape[0]*train_test_sparse.shape[1]*64/8/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно сократить размер ячейки до 8 бит, но всеравно неразряженная матрица будет большой (15 Gb). Поэтому имеется смысл использования csr  матрицы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_sparse.shape[0]*train_test_sparse.shape[1]*8/8/1024**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sparse = train_test_sparse.astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = train_df.shape[0]\n",
    "X_train_sparse = train_test_sparse[:train_len]\n",
    "X_test_sparse = train_test_sparse[train_len:]\n",
    "y = train_df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Вопрос 1. </font> \n",
    "**Выведите размерности матриц *X_train_sparse* и *X_test_sparse* – 4 числа на одной строке через пробел: число строк и столбцов матрицы *X_train_sparse*, затем число строк и столбцов матрицы *X_test_sparse*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'253561 48371 82797 48371'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([f\"{a} {b}\" for a, b in [X_train_sparse.shape, X_test_sparse.shape]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *kaggle_data/train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_test_sparse.pkl'), 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_target.pkl'), 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разобьем обучающую выборку на 2 части в пропорции 7/3, причем не перемешивая. Исходные данные упорядочены по времени, тестовая выборка по времени четко отделена от обучающей, это же соблюдем и здесь.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_share = int(.7 * X_train_sparse.shape[0])\n",
    "X_train, y_train = X_train_sparse[:train_share, :], y[:train_share]\n",
    "X_valid, y_valid = X_train_sparse[train_share:, :], y[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD_logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создайте объект `sklearn.linear_model.SGDClassifier` с логистической функцией потерь и параметром *random_state*=17. Остальные параметры оставьте по умолчанию, разве что *n_jobs*=-1 никогда не помешает. Обучите  модель на выборке `(X_train, y_train)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
       "              random_state=17, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей того, что это сессия Элис, на отложенной выборке *(X_valid, y_valid)*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_valid_pred_proba = sgd_logit.predict_proba(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_logit.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76069, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_valid_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.11911413e-01, 8.80885865e-02],\n",
       "       [9.99703507e-01, 2.96493343e-04],\n",
       "       [9.99872654e-01, 1.27345522e-04],\n",
       "       ...,\n",
       "       [9.87751880e-01, 1.22481202e-02],\n",
       "       [9.99943934e-01, 5.60663172e-05],\n",
       "       [9.88685254e-01, 1.13147460e-02]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_valid_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD_logit AUC score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Вопрос 2. </font> \n",
    "**Посчитайте ROC AUC логистической регрессии, обученной с помощью стохастического градиентного спуска, на отложенной выборке. Округлите до 3 знаков после разделителя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score: 0.934\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_valid, logit_valid_pred_proba[:, 1])\n",
    "print(f\"AUC score: {auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз в виде предсказанных вероятностей отнесения к классу 1 для тестовой выборки с помощью той же *sgd_logit*, обученной уже на всей обучающей выборке (а не на 70%).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 685 ms, sys: 259 µs, total: 685 ms\n",
      "Wall time: 607 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit = SGDClassifier(random_state=17, n_jobs=-1, loss='log')\n",
    "sgd_logit.fit(X_train_sparse, y)\n",
    "logit_test_pred_proba = sgd_logit.predict_proba(X_test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] Coursera_Username\", чтоб можно было легко идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2/leaderboard/public).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index=np.arange(\n",
    "                                    1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred_proba[:, 1], \"out.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.24441919e-02, 5.64920887e-05, 1.81927605e-05, ...,\n",
       "       1.17325100e-02, 5.00373821e-03, 1.04058395e-03])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_test_pred_proba[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Public score 0.91646**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_test_df\n",
    "del train_test_df_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector plus ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort data by time for correct validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max value for site id = 48371.0 < 2^16\n",
      "(336358, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>784</td>\n",
       "      <td>949</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>948</td>\n",
       "      <td>949</td>\n",
       "      <td>948</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "      <td>945</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>948</td>\n",
       "      <td>947</td>\n",
       "      <td>950</td>\n",
       "      <td>952</td>\n",
       "      <td>946</td>\n",
       "      <td>951</td>\n",
       "      <td>946</td>\n",
       "      <td>947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "21669          56     55      0      0      0      0      0      0      0   \n",
       "54843          56     55     56     55      0      0      0      0      0   \n",
       "77292         946    946    951    946    946    945    948    784    949   \n",
       "114021        945    948    949    948    945    946    947    945    946   \n",
       "146670        947    950    948    947    950    952    946    951    946   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "21669            0  \n",
       "54843            0  \n",
       "77292          946  \n",
       "114021         946  \n",
       "146670         947  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = ['site%d' % i for i in range(1, 11)]\n",
    "\n",
    "train = train_df.sort_values(by='time1')\n",
    "y = train['target'].astype('uint8').values\n",
    "train_test = pd.concat([train, test_df]).fillna(0)\n",
    "\n",
    "print(\"Max value for site id = {} < 2^16\".format(max(train_test[sites].max())))\n",
    "\n",
    "train_test_sites = train_test[sites].astype('uint16')\n",
    "\n",
    "print(train_test_sites.shape)\n",
    "\n",
    "train_test_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ngrams to vector of sites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split on train/test/train_part/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_trainpart_valid(X, Y):\n",
    "    n = Y.shape[0]\n",
    "    n_part = int(0.7*n)\n",
    "    train, test = X[:n],  X[n:]\n",
    "    train_part, y_part = train[:n_part], Y[:n_part]\n",
    "    valid, y_valid = train[n_part:], Y[n_part:]\n",
    "    return train, test, train_part, y_part, valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **SGD_logit** sites plus ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(model, X, Y):\n",
    "    # train model on train_part and get score on validation\n",
    "    print(f\"Number of features : {X.shape[1]}\")\n",
    "    train, test,  train_part, y_part, valid, y_valid = \\\n",
    "        get_train_test_trainpart_valid(X, Y)\n",
    "    model.fit(train_part, y_part)\n",
    "    predictions = model.predict_proba(valid)\n",
    "    auc = roc_auc_score(y_valid, predictions[:, 1])\n",
    "    print(f\"AUC score on valid part: {auc:.4f}\")\n",
    "\n",
    "    # train model on train set and get cv score\n",
    "    time_split = TimeSeriesSplit(n_splits=6)\n",
    "    model.fit(train, Y)\n",
    "    cv_scores = cross_val_score(model, train, Y, cv=time_split,\n",
    "                                scoring='roc_auc', n_jobs=1)\n",
    "    print(\n",
    "        f\"AUC score on cross-val: {cv_scores.mean():.4f} {cv_scores.std():.4f}\\n\")\n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_assessment(model, X, Y):\n",
    "    # Scores on X\n",
    "    cross_val(model, X, Y)\n",
    "\n",
    "    # Selection features\n",
    "    print(\"Selection :\")\n",
    "    selector = SelectFromModel(model, max_features=min(500, X.shape[1])).fit(X[:Y.shape[0]], Y)\n",
    "    X_transformed=selector.transform(X)\n",
    "\n",
    "    # Scores on X-transformed\n",
    "    cross_val(model, X_transformed, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(random_state=17, n_jobs=-1, loss='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка Ngram и количества признаков, на которых модель выглядит лучше по AUC валидации и кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector plus (1,1) ngrams :\n",
      "Number of features : 48371\n",
      "AUC score on valid part: 0.8604\n",
      "AUC score on cross-val: 0.8306 0.0909\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 12%|█▎        | 1/8 [00:22<02:40, 22.92s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8528 0.0920\n",
      "\n",
      "Vector plus (1,2) ngrams :\n",
      "Number of features : 432875\n",
      "AUC score on valid part: 0.8616\n",
      "AUC score on cross-val: 0.8359 0.0944\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 25%|██▌       | 2/8 [00:57<02:38, 26.37s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8701 0.0657\n",
      "\n",
      "Vector plus (1,3) ngrams :\n",
      "Number of features : 1288967\n",
      "AUC score on valid part: 0.8590\n",
      "AUC score on cross-val: 0.8351 0.0986\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 38%|███▊      | 3/8 [01:41<02:38, 31.76s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8744 0.0626\n",
      "\n",
      "Vector plus (1,4) ngrams :\n",
      "Number of features : 2444485\n",
      "AUC score on valid part: 0.8591\n",
      "AUC score on cross-val: 0.8338 0.1007\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 4/8 [02:41<02:40, 40.25s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8746 0.0627\n",
      "\n",
      "Vector plus (1,5) ngrams :\n",
      "Number of features : 3688818\n",
      "AUC score on valid part: 0.8587\n",
      "AUC score on cross-val: 0.8330 0.1020\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 62%|██████▎   | 5/8 [03:56<02:31, 50.62s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8741 0.0628\n",
      "\n",
      "Vector plus (1,6) ngrams :\n",
      "Number of features : 4863251\n",
      "AUC score on valid part: 0.8583\n",
      "AUC score on cross-val: 0.8311 0.1055\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 6/8 [05:26<02:05, 62.56s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8740 0.0627\n",
      "\n",
      "Vector plus (1,7) ngrams :\n",
      "Number of features : 5871108\n",
      "AUC score on valid part: 0.8576\n",
      "AUC score on cross-val: 0.8313 0.1043\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 7/8 [07:11<01:15, 75.04s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8740 0.0628\n",
      "\n",
      "Vector plus (1,8) ngrams :\n",
      "Number of features : 6659432\n",
      "AUC score on valid part: 0.8572\n",
      "AUC score on cross-val: 0.8291 0.1061\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "100%|██████████| 8/8 [09:10<00:00, 68.82s/it]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score on cross-val: 0.8741 0.0628\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1, 9)):\n",
    "    print(f\"Vector plus (1,{i}) ngrams :\")\n",
    "    train_test_sparse, _ = get_sparse(train_test_sites.values, nr=(1, i))\n",
    "    model_assessment(sgd_logit, train_test_sparse, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кросс-валидация на большом количестве признаков падает, много шумовых признаков.\n",
    "После выборки признаков по модели, качество возрастает. Ngrams дают новые признаки, на которых модель выдает лучшие оценки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 4863251\n",
      "AUC score on valid part: 0.8583\n",
      "AUC score on cross-val: 0.8311 0.1055\n",
      "\n",
      "Selection :\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8979\n",
      "AUC score on cross-val: 0.8740 0.0627\n",
      "\n",
      "Number of features : 400\n",
      "AUC score on valid part: 0.8931\n",
      "AUC score on cross-val: 0.8738 0.0668\n",
      "\n",
      "Number of features : 410\n",
      "AUC score on valid part: 0.8955\n",
      "AUC score on cross-val: 0.8729 0.0629\n",
      "\n",
      "Number of features : 420\n",
      "AUC score on valid part: 0.8942\n",
      "AUC score on cross-val: 0.8723 0.0625\n",
      "\n",
      "Number of features : 430\n",
      "AUC score on valid part: 0.8943\n",
      "AUC score on cross-val: 0.8735 0.0607\n",
      "\n",
      "Number of features : 440\n",
      "AUC score on valid part: 0.8949\n",
      "AUC score on cross-val: 0.8728 0.0623\n",
      "\n",
      "Number of features : 450\n",
      "AUC score on valid part: 0.8955\n",
      "AUC score on cross-val: 0.8733 0.0628\n",
      "\n",
      "Number of features : 460\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8741 0.0628\n",
      "\n",
      "Number of features : 470\n",
      "AUC score on valid part: 0.8975\n",
      "AUC score on cross-val: 0.8742 0.0627\n",
      "\n",
      "Number of features : 480\n",
      "AUC score on valid part: 0.8976\n",
      "AUC score on cross-val: 0.8741 0.0625\n",
      "\n",
      "Number of features : 490\n",
      "AUC score on valid part: 0.8979\n",
      "AUC score on cross-val: 0.8745 0.0622\n",
      "\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8979\n",
      "AUC score on cross-val: 0.8740 0.0627\n",
      "\n",
      "Number of features : 510\n",
      "AUC score on valid part: 0.8978\n",
      "AUC score on cross-val: 0.8740 0.0630\n",
      "\n",
      "Number of features : 520\n",
      "AUC score on valid part: 0.8972\n",
      "AUC score on cross-val: 0.8737 0.0640\n",
      "\n",
      "Number of features : 530\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8738 0.0639\n",
      "\n",
      "Number of features : 540\n",
      "AUC score on valid part: 0.8975\n",
      "AUC score on cross-val: 0.8741 0.0637\n",
      "\n",
      "Number of features : 550\n",
      "AUC score on valid part: 0.8951\n",
      "AUC score on cross-val: 0.8710 0.0678\n",
      "\n",
      "Number of features : 560\n",
      "AUC score on valid part: 0.8952\n",
      "AUC score on cross-val: 0.8715 0.0678\n",
      "\n",
      "Number of features : 570\n",
      "AUC score on valid part: 0.8938\n",
      "AUC score on cross-val: 0.8705 0.0683\n",
      "\n",
      "Number of features : 580\n",
      "AUC score on valid part: 0.8939\n",
      "AUC score on cross-val: 0.8707 0.0682\n",
      "\n",
      "Number of features : 590\n",
      "AUC score on valid part: 0.8941\n",
      "AUC score on cross-val: 0.8711 0.0683\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_test_sparse, _ = get_sparse(train_test_sites.values, nr=(1, 6))\n",
    "model_assessment(sgd_logit, train_test_sparse, y)\n",
    "X, Y = train_test_sparse, y\n",
    "model = sgd_logit\n",
    "\n",
    "for nf in np.arange(400, 600, 10):\n",
    "    selector = SelectFromModel(model, max_features=min(\n",
    "        nf, X.shape[1])).fit(X[:Y.shape[0]], Y)\n",
    "    X_transformed = selector.transform(X)\n",
    "    scores = cross_val(model, X_transformed, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерно 490 признаков дают хорошую оценку на кросс-валидации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор признаков, на которых модель работает лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 486325\n",
      "AUC score on valid part: 0.8587\n",
      "AUC score on cross-val: 0.8323 0.1035\n",
      "\n",
      "Number of features : 48632\n",
      "AUC score on valid part: 0.8657\n",
      "AUC score on cross-val: 0.8397 0.0978\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_test_sparse, y\n",
    "model = sgd_logit\n",
    "nf = int(X.shape[1]/10)\n",
    "while(nf > 10000):\n",
    "    selector = SelectFromModel(model, max_features=nf).fit(X[:Y.shape[0]], Y)\n",
    "    X = selector.transform(X)\n",
    "    cross_val(model, X, Y)\n",
    "    nf = int(X.shape[1]/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 400\n",
      "AUC score on valid part: 0.8929\n",
      "AUC score on cross-val: 0.8736 0.0667\n",
      "\n",
      "Number of features : 410\n",
      "AUC score on valid part: 0.8935\n",
      "AUC score on cross-val: 0.8740 0.0667\n",
      "\n",
      "Number of features : 420\n",
      "AUC score on valid part: 0.8939\n",
      "AUC score on cross-val: 0.8737 0.0646\n",
      "\n",
      "Number of features : 430\n",
      "AUC score on valid part: 0.8943\n",
      "AUC score on cross-val: 0.8734 0.0647\n",
      "\n",
      "Number of features : 440\n",
      "AUC score on valid part: 0.8964\n",
      "AUC score on cross-val: 0.8740 0.0611\n",
      "\n",
      "Number of features : 450\n",
      "AUC score on valid part: 0.8954\n",
      "AUC score on cross-val: 0.8740 0.0609\n",
      "\n",
      "Number of features : 460\n",
      "AUC score on valid part: 0.8956\n",
      "AUC score on cross-val: 0.8741 0.0608\n",
      "\n",
      "Number of features : 470\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8750 0.0608\n",
      "\n",
      "Number of features : 480\n",
      "AUC score on valid part: 0.8974\n",
      "AUC score on cross-val: 0.8749 0.0618\n",
      "\n",
      "Number of features : 490\n",
      "AUC score on valid part: 0.8971\n",
      "AUC score on cross-val: 0.8745 0.0630\n",
      "\n",
      "Number of features : 500\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8741 0.0632\n",
      "\n",
      "Number of features : 510\n",
      "AUC score on valid part: 0.8971\n",
      "AUC score on cross-val: 0.8741 0.0636\n",
      "\n",
      "Number of features : 520\n",
      "AUC score on valid part: 0.8968\n",
      "AUC score on cross-val: 0.8732 0.0648\n",
      "\n",
      "Number of features : 530\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8738 0.0638\n",
      "\n",
      "Number of features : 540\n",
      "AUC score on valid part: 0.8976\n",
      "AUC score on cross-val: 0.8739 0.0637\n",
      "\n",
      "Number of features : 550\n",
      "AUC score on valid part: 0.8963\n",
      "AUC score on cross-val: 0.8730 0.0633\n",
      "\n",
      "Number of features : 560\n",
      "AUC score on valid part: 0.8963\n",
      "AUC score on cross-val: 0.8734 0.0632\n",
      "\n",
      "Number of features : 570\n",
      "AUC score on valid part: 0.8965\n",
      "AUC score on cross-val: 0.8703 0.0717\n",
      "\n",
      "Number of features : 580\n",
      "AUC score on valid part: 0.8943\n",
      "AUC score on cross-val: 0.8696 0.0713\n",
      "\n",
      "Number of features : 590\n",
      "AUC score on valid part: 0.8946\n",
      "AUC score on cross-val: 0.8695 0.0720\n",
      "\n",
      "Number of features : 600\n",
      "AUC score on valid part: 0.8942\n",
      "AUC score on cross-val: 0.8678 0.0762\n",
      "\n",
      "Number of features : 610\n",
      "AUC score on valid part: 0.8941\n",
      "AUC score on cross-val: 0.8680 0.0764\n",
      "\n",
      "Number of features : 620\n",
      "AUC score on valid part: 0.8943\n",
      "AUC score on cross-val: 0.8683 0.0764\n",
      "\n",
      "Number of features : 630\n",
      "AUC score on valid part: 0.8943\n",
      "AUC score on cross-val: 0.8684 0.0763\n",
      "\n",
      "Number of features : 640\n",
      "AUC score on valid part: 0.8936\n",
      "AUC score on cross-val: 0.8687 0.0762\n",
      "\n",
      "Number of features : 650\n",
      "AUC score on valid part: 0.8935\n",
      "AUC score on cross-val: 0.8686 0.0765\n",
      "\n",
      "Number of features : 660\n",
      "AUC score on valid part: 0.8925\n",
      "AUC score on cross-val: 0.8683 0.0750\n",
      "\n",
      "Number of features : 670\n",
      "AUC score on valid part: 0.8925\n",
      "AUC score on cross-val: 0.8683 0.0750\n",
      "\n",
      "Number of features : 680\n",
      "AUC score on valid part: 0.8928\n",
      "AUC score on cross-val: 0.8684 0.0749\n",
      "\n",
      "Number of features : 690\n",
      "AUC score on valid part: 0.8928\n",
      "AUC score on cross-val: 0.8684 0.0746\n",
      "\n",
      "Number of features : 700\n",
      "AUC score on valid part: 0.8926\n",
      "AUC score on cross-val: 0.8686 0.0745\n",
      "\n",
      "Number of features : 710\n",
      "AUC score on valid part: 0.8923\n",
      "AUC score on cross-val: 0.8685 0.0745\n",
      "\n",
      "Number of features : 720\n",
      "AUC score on valid part: 0.8954\n",
      "AUC score on cross-val: 0.8636 0.0896\n",
      "\n",
      "Number of features : 730\n",
      "AUC score on valid part: 0.8954\n",
      "AUC score on cross-val: 0.8636 0.0899\n",
      "\n",
      "Number of features : 740\n",
      "AUC score on valid part: 0.8955\n",
      "AUC score on cross-val: 0.8638 0.0899\n",
      "\n",
      "Number of features : 750\n",
      "AUC score on valid part: 0.8953\n",
      "AUC score on cross-val: 0.8630 0.0892\n",
      "\n",
      "Number of features : 760\n",
      "AUC score on valid part: 0.8953\n",
      "AUC score on cross-val: 0.8630 0.0892\n",
      "\n",
      "Number of features : 770\n",
      "AUC score on valid part: 0.8952\n",
      "AUC score on cross-val: 0.8630 0.0892\n",
      "\n",
      "Number of features : 780\n",
      "AUC score on valid part: 0.8950\n",
      "AUC score on cross-val: 0.8631 0.0890\n",
      "\n",
      "Number of features : 790\n",
      "AUC score on valid part: 0.8950\n",
      "AUC score on cross-val: 0.8629 0.0894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# X, Y = train_test_sparse, y\n",
    "grid = []\n",
    "for nf in np.arange(400, 800, 10):\n",
    "    selector = SelectFromModel(model, max_features=nf).fit(X[:Y.shape[0]], Y)\n",
    "    X_transformed = selector.transform(X)\n",
    "    scores = cross_val(model, X_transformed, Y)\n",
    "    grid.append((scores.mean(), scores.std(), nf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8749522922274275, 0.060766084849902646, 470)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(grid)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(model, max_features=470).fit(X[:Y.shape[0]], Y)\n",
    "X_transformed = selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 470\n",
      "AUC score on valid part: 0.8973\n",
      "AUC score on cross-val: 0.8750 0.0608\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.74457889, 0.90671923, 0.87453794, 0.90817977, 0.88687132,\n",
       "       0.9288266 ])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val(sgd_logit, X_transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test,  train_part, y_part, valid, y_valid = \\\n",
    "    get_train_test_trainpart_valid(X_transformed, Y)\n",
    "\n",
    "model.fit(train, Y)\n",
    "proba = model.predict_proba(test)\n",
    "\n",
    "write_to_submission_file(proba[:, 1], \"out1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Public score : 0.92017**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF convertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf конвертирует таблицу векторов согласно значимости термов. Частые термы, встречающиеся во всех документах уменьшит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 3688818\n",
      "AUC score on valid part: 0.8699\n",
      "AUC score on cross-val: 0.8084 0.0957\n",
      "\n",
      "Selection :\n",
      "Number of features : 40000\n",
      "AUC score on valid part: 0.8717\n",
      "AUC score on cross-val: 0.8087 0.0962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfTransformer()\n",
    "train_test_sparse, _ = get_sparse(train_test_sites.values, nr=(1, 5))\n",
    "train_test_sparse = tfid.fit_transform(train_test_sparse)\n",
    "model_assessment(sgd_logit, train_test_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features : 500\n",
      "AUC score on valid part: 0.8666\n",
      "AUC score on cross-val: 0.8012 0.0988\n",
      "\n",
      "Number of features : 1000\n",
      "AUC score on valid part: 0.8685\n",
      "AUC score on cross-val: 0.8049 0.0976\n",
      "\n",
      "Number of features : 1500\n",
      "AUC score on valid part: 0.8696\n",
      "AUC score on cross-val: 0.8052 0.0970\n",
      "\n",
      "Number of features : 2000\n",
      "AUC score on valid part: 0.8702\n",
      "AUC score on cross-val: 0.8061 0.0969\n",
      "\n",
      "Number of features : 2500\n",
      "AUC score on valid part: 0.8703\n",
      "AUC score on cross-val: 0.8066 0.0967\n",
      "\n",
      "Number of features : 3000\n",
      "AUC score on valid part: 0.8708\n",
      "AUC score on cross-val: 0.8071 0.0968\n",
      "\n",
      "Number of features : 3500\n",
      "AUC score on valid part: 0.8708\n",
      "AUC score on cross-val: 0.8072 0.0966\n",
      "\n",
      "Number of features : 4000\n",
      "AUC score on valid part: 0.8712\n",
      "AUC score on cross-val: 0.8076 0.0966\n",
      "\n",
      "Number of features : 4500\n",
      "AUC score on valid part: 0.8713\n",
      "AUC score on cross-val: 0.8078 0.0965\n",
      "\n",
      "Number of features : 5000\n",
      "AUC score on valid part: 0.8714\n",
      "AUC score on cross-val: 0.8079 0.0965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_test_sparse, y\n",
    "model = sgd_logit\n",
    "for nf in np.arange(500, 5001, 500):\n",
    "    selector = SelectFromModel(model, max_features=min(\n",
    "        nf, X.shape[1])).fit(X[:Y.shape[0]], Y)\n",
    "    X_transformed = selector.transform(X)\n",
    "    cross_val(model, X_transformed, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf не подходит для данного набора векторов. Не частые посещения сайтов не определяют поведение. Поведение это выделение признаков, которые повторяются из раза в раз."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logit** sites plus  ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort df by start date. Get info for visited sites. Convert visited sites to sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% % time\n",
    "logit = LogisticRegression(C=1, random_state=17, solver='liblinear')\n",
    "logit.fit(train_sites, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sparse. Add ngrams (range up to 3) to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% % time\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "cv_scores2 = cross_val_score(logit, train_sites, y, cv=time_split,\n",
    "                             scoring='roc_auc', n_jobs=1)\n",
    "cv_scores2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores2.mean(), cv_scores2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(logit.predict_proba(test_sites)[:, 1], \"out2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Public score : 0.91395**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Counting without '0's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.array([[1]*2+[0]*3,\n",
    "                 [3]*3+[1]*2,\n",
    "                 [3]+[1]+[3]+[0]*2,\n",
    "                 [10]*3+[0]*2])\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\" \".join(map(str, row)) for row in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), token_pattern='\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words_transformed = vectorizer.fit_transform(\n",
    "    [\" \".join(map(str, row)) for row in docs])\n",
    "words_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.vocabulary_\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in voc.keys():\n",
    "    print(f\"{k} : {re.findall(r'(^0$)|( 0 )|(^0 )' , k)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = [k for k in voc.keys() if (len(re.findall(r'(^0$)|( 0 )|(^0 )', k)))]\n",
    "for k in remove:\n",
    "    del voc[k]\n",
    "\n",
    "ind = 0\n",
    "for k in voc:\n",
    "    voc[k] = ind\n",
    "    ind += 1\n",
    "voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(\n",
    "    1, 2), token_pattern='\\w+', vocabulary=voc)\n",
    "words_transformed = vectorizer.fit_transform(\n",
    "    [\" \".join(map(str, row)) for row in docs])\n",
    "words_transformed.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии оценки работы (только для Peer Review в специализации):\n",
    "- Правильные ли получились размерности матриц в п. 1? (max. 2 балла)\n",
    "- Правильным ли получилось значения ROC AUC в п. 2? (max. 4 балла)\n",
    "- Побит ли бенчмарк \"sgd_logit_benchmark.csv\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)\n",
    "- Побит ли бенчмарк \"Logit +3 features\" на публичной части рейтинга в соревновании Kaggle? (max. 2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **sgd_logit_benchmark.csv 0.91273**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logit +3 features Yury Kashnitsky master 0.92784**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в описание финального проекта (`html`, `pdf` или `ipynb`). Это только в случае, если вы проходите специализацию.\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям, отделив одного из пользователей от остальных – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов, также хороша [статья](https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending) Александра Дьяконова\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных веб-страницах Элис и остальными 1557 пользователями (*train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем большой тьюториал по Vowpal Wabbit и попробуем его в деле, на данных соревнования."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "417.129px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
