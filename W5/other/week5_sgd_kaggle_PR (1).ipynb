{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://habrastorage.org/web/677/8e1/337/6778e1337c3d4b159d7e99df94227cb2.jpg\"/>\n",
    "## Специализация \"Машинное обучение и анализ данных\"\n",
    "</center>\n",
    "<center>Автор материала: программист-исследователь Mail.ru Group, старший преподаватель Факультета Компьютерных Наук ВШЭ Юрий Кашницкий"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Capstone проект №1. Идентификация пользователей по посещенным веб-страницам\n",
    "<img src='http://i.istockimg.com/file_thumbview_approve/21546327/5/stock-illustration-21546327-identification-de-l-utilisateur.jpg'>\n",
    "\n",
    "# <center>Неделя 5.  Соревнование Kaggle \"Identify Me If You Can\"\n",
    "\n",
    "На этой неделе мы вспомним про концепцию стохастического градиентного спуска и опробуем классификатор Scikit-learn SGDClassifier, который работает намного быстрее на больших выборках, чем алгоритмы, которые мы тестировали на 4 неделе. Также мы познакомимся с данными [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can4) Kaggle по идентификации пользователей и сделаем в нем первые посылки. По итогам этой недели дополнительные баллы получат те, кто попадет в топ-30 публичного лидерборда соревнования.\n",
    "\n",
    "**В этой части проекта Вам могут быть полезны видеозаписи следующих лекций курса \"Обучение на размеченных данных\":**\n",
    "   - [Стохатический градиентный спуск](https://www.coursera.org/learn/supervised-learning/lecture/xRY50/stokhastichieskii-ghradiientnyi-spusk)\n",
    "   - [Линейные модели. Sklearn.linear_model. Классификация](https://www.coursera.org/learn/supervised-learning/lecture/EBg9t/linieinyie-modieli-sklearn-linear-model-klassifikatsiia)\n",
    "   \n",
    "**Также рекомендуется вернуться и просмотреть [задание](https://www.coursera.org/learn/supervised-learning/programming/t2Idc/linieinaia-rieghriessiia-i-stokhastichieskii-ghradiientnyi-spusk) \"Линейная регрессия и стохастический градиентный спуск\" 1 недели 2 курса специализации.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.sparse\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import xgboost\n",
    "import lightgbm\n",
    "import timeit\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/identify-me-if-you-can4/data) в DataFrame train_df и test_df (обучающая и тестовая выборки).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = '../data/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>2014-03-24 15:22:40</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:48</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>23720.0</td>\n",
       "      <td>2014-03-24 15:22:54</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-24 15:22:55</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:01</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:03</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:04</td>\n",
       "      <td>23713.0</td>\n",
       "      <td>2014-03-24 15:23:05</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>2014-04-17 14:25:58</td>\n",
       "      <td>8725.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>665.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>8727.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:25:59</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2014-04-17 14:26:01</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:18</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:47</td>\n",
       "      <td>5320.0</td>\n",
       "      <td>2014-04-17 14:26:48</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>2014-03-21 10:12:24</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2014-03-21 10:12:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:12:54</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:24</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-03-21 10:13:36</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:13:54</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-03-21 10:14:01</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:06</td>\n",
       "      <td>303.0</td>\n",
       "      <td>2014-03-21 10:14:24</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>2013-12-13 09:52:28</td>\n",
       "      <td>925.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1240.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1360.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:54:34</td>\n",
       "      <td>1344.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>1345.0</td>\n",
       "      <td>2013-12-13 09:58:19</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>2013-11-26 12:35:29</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2013-11-26 12:35:31</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-11-26 12:35:32</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2013-11-26 12:37:03</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2013-11-26 12:37:04</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1    site2                time2    site3  \\\n",
       "session_id                                                                      \n",
       "1           23713  2014-03-24 15:22:40  23720.0  2014-03-24 15:22:48  23713.0   \n",
       "2            8726  2014-04-17 14:25:58   8725.0  2014-04-17 14:25:59    665.0   \n",
       "3             303  2014-03-21 10:12:24     19.0  2014-03-21 10:12:36    303.0   \n",
       "4            1359  2013-12-13 09:52:28    925.0  2013-12-13 09:54:34   1240.0   \n",
       "5              11  2013-11-26 12:35:29     85.0  2013-11-26 12:35:31     52.0   \n",
       "\n",
       "                          time3    site4                time4    site5  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:22:48  23713.0  2014-03-24 15:22:54  23720.0   \n",
       "2           2014-04-17 14:25:59   8727.0  2014-04-17 14:25:59     45.0   \n",
       "3           2014-03-21 10:12:54    303.0  2014-03-21 10:13:01    303.0   \n",
       "4           2013-12-13 09:54:34   1360.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:35:31     85.0  2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time5   ...                  time6    site7  \\\n",
       "session_id                        ...                                   \n",
       "1           2014-03-24 15:22:54   ...    2014-03-24 15:22:55  23713.0   \n",
       "2           2014-04-17 14:25:59   ...    2014-04-17 14:26:01     45.0   \n",
       "3           2014-03-21 10:13:24   ...    2014-03-21 10:13:36    303.0   \n",
       "4           2013-12-13 09:54:34   ...    2013-12-13 09:54:34   1346.0   \n",
       "5           2013-11-26 12:35:32   ...    2013-11-26 12:35:32     11.0   \n",
       "\n",
       "                          time7    site8                time8    site9  \\\n",
       "session_id                                                               \n",
       "1           2014-03-24 15:23:01  23713.0  2014-03-24 15:23:03  23713.0   \n",
       "2           2014-04-17 14:26:01   5320.0  2014-04-17 14:26:18   5320.0   \n",
       "3           2014-03-21 10:13:54    309.0  2014-03-21 10:14:01    303.0   \n",
       "4           2013-12-13 09:54:34   1345.0  2013-12-13 09:54:34   1344.0   \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:03     10.0   \n",
       "\n",
       "                          time9   site10               time10 user_id  \n",
       "session_id                                                             \n",
       "1           2014-03-24 15:23:04  23713.0  2014-03-24 15:23:05     653  \n",
       "2           2014-04-17 14:26:47   5320.0  2014-04-17 14:26:48     198  \n",
       "3           2014-03-21 10:14:06    303.0  2014-03-21 10:14:24      34  \n",
       "4           2013-12-13 09:58:19   1345.0  2013-12-13 09:58:19     601  \n",
       "5           2013-11-26 12:37:03     85.0  2013-11-26 12:37:04     273  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Объединим обучающую и тестовую выборки – это понадобится, чтоб вместе потом привести их к разреженному формату.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающей выборке видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длиннее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на статистику признаков.**\n",
    "\n",
    "Пропуски возникают там, где сессии короткие (менее 10 сайтов). Скажем, если человек 1 января 2015 года посетил *vk.com* в 20:01, потом *yandex.ru* в 20:29, затем *google.com* в 20:33, то первая его сессия будет состоять только из двух сайтов (site1 – ID сайта *vk.com*, time1 – 2015-01-01 20:01:00, site2 – ID сайта  *yandex.ru*, time2 – 2015-01-01 20:29:00, остальные признаки – NaN), а начиная с *google.com* пойдет новая сессия, потому что уже прошло более 30 минут с момента посещения *vk.com*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 182793 entries, 1 to 182793\n",
      "Data columns (total 21 columns):\n",
      "site1      182793 non-null int64\n",
      "time1      182793 non-null object\n",
      "site2      181175 non-null float64\n",
      "time2      181175 non-null object\n",
      "site3      179441 non-null float64\n",
      "time3      179441 non-null object\n",
      "site4      178054 non-null float64\n",
      "time4      178054 non-null object\n",
      "site5      176653 non-null float64\n",
      "time5      176653 non-null object\n",
      "site6      175268 non-null float64\n",
      "time6      175268 non-null object\n",
      "site7      173960 non-null float64\n",
      "time7      173960 non-null object\n",
      "site8      172738 non-null float64\n",
      "time8      172738 non-null object\n",
      "site9      171437 non-null float64\n",
      "time9      171437 non-null object\n",
      "site10     170247 non-null float64\n",
      "time10     170247 non-null object\n",
      "user_id    182793 non-null int64\n",
      "dtypes: float64(9), int64(2), object(10)\n",
      "memory usage: 30.7+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>2014-10-04 12:24:43</td>\n",
       "      <td>304.0</td>\n",
       "      <td>2014-10-04 12:25:34</td>\n",
       "      <td>308.0</td>\n",
       "      <td>2014-10-04 12:28:33</td>\n",
       "      <td>307.0</td>\n",
       "      <td>2014-10-04 12:28:33</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2014-10-04 12:28:33</td>\n",
       "      <td>308.0</td>\n",
       "      <td>2014-10-04 12:28:34</td>\n",
       "      <td>312.0</td>\n",
       "      <td>2014-10-04 12:30:31</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2014-10-04 12:31:30</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2014-10-04 12:32:31</td>\n",
       "      <td>309.0</td>\n",
       "      <td>2014-10-04 12:34:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>838</td>\n",
       "      <td>2014-12-02 09:20:37</td>\n",
       "      <td>504.0</td>\n",
       "      <td>2014-12-02 09:20:38</td>\n",
       "      <td>68.0</td>\n",
       "      <td>2014-12-02 09:20:38</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-12-02 09:20:38</td>\n",
       "      <td>838.0</td>\n",
       "      <td>2014-12-02 09:20:38</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2014-12-02 09:20:40</td>\n",
       "      <td>838.0</td>\n",
       "      <td>2014-12-02 09:20:41</td>\n",
       "      <td>886.0</td>\n",
       "      <td>2014-12-02 09:20:42</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2014-12-02 09:20:42</td>\n",
       "      <td>305.0</td>\n",
       "      <td>2014-12-02 09:20:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>190</td>\n",
       "      <td>2014-10-01 09:27:38</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2014-10-01 09:27:38</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-10-01 09:27:38</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2014-10-01 09:27:38</td>\n",
       "      <td>191.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "      <td>189.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "      <td>192.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2014-10-01 09:27:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>295</td>\n",
       "      <td>2014-10-02 14:34:17</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:18</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:19</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:20</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:21</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:22</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:23</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:24</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:25</td>\n",
       "      <td>295.0</td>\n",
       "      <td>2014-10-02 14:34:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31</td>\n",
       "      <td>2014-05-19 17:50:21</td>\n",
       "      <td>3177.0</td>\n",
       "      <td>2014-05-19 17:50:21</td>\n",
       "      <td>3174.0</td>\n",
       "      <td>2014-05-19 17:50:21</td>\n",
       "      <td>32434.0</td>\n",
       "      <td>2014-05-19 17:50:22</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2014-05-19 17:50:22</td>\n",
       "      <td>31.0</td>\n",
       "      <td>2014-05-19 17:50:22</td>\n",
       "      <td>5698.0</td>\n",
       "      <td>2014-05-19 17:50:25</td>\n",
       "      <td>5698.0</td>\n",
       "      <td>2014-05-19 17:50:26</td>\n",
       "      <td>5698.0</td>\n",
       "      <td>2014-05-19 17:50:27</td>\n",
       "      <td>4173.0</td>\n",
       "      <td>2014-05-19 17:50:27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1                time1   site2                time2   site3  \\\n",
       "session_id                                                                    \n",
       "1               9  2014-10-04 12:24:43   304.0  2014-10-04 12:25:34   308.0   \n",
       "2             838  2014-12-02 09:20:37   504.0  2014-12-02 09:20:38    68.0   \n",
       "3             190  2014-10-01 09:27:38   192.0  2014-10-01 09:27:38     8.0   \n",
       "4             295  2014-10-02 14:34:17   295.0  2014-10-02 14:34:18   295.0   \n",
       "5              31  2014-05-19 17:50:21  3177.0  2014-05-19 17:50:21  3174.0   \n",
       "\n",
       "                          time3    site4                time4  site5  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 12:28:33    307.0  2014-10-04 12:28:33   91.0   \n",
       "2           2014-12-02 09:20:38     11.0  2014-12-02 09:20:38  838.0   \n",
       "3           2014-10-01 09:27:38    189.0  2014-10-01 09:27:38  191.0   \n",
       "4           2014-10-02 14:34:19    295.0  2014-10-02 14:34:20  295.0   \n",
       "5           2014-05-19 17:50:21  32434.0  2014-05-19 17:50:22   27.0   \n",
       "\n",
       "                          time5  site6                time6   site7  \\\n",
       "session_id                                                            \n",
       "1           2014-10-04 12:28:33  308.0  2014-10-04 12:28:34   312.0   \n",
       "2           2014-12-02 09:20:38   11.0  2014-12-02 09:20:40   838.0   \n",
       "3           2014-10-01 09:27:39  189.0  2014-10-01 09:27:39   190.0   \n",
       "4           2014-10-02 14:34:21  295.0  2014-10-02 14:34:22   295.0   \n",
       "5           2014-05-19 17:50:22   31.0  2014-05-19 17:50:22  5698.0   \n",
       "\n",
       "                          time7   site8                time8   site9  \\\n",
       "session_id                                                             \n",
       "1           2014-10-04 12:30:31   300.0  2014-10-04 12:31:30   305.0   \n",
       "2           2014-12-02 09:20:41   886.0  2014-12-02 09:20:42    27.0   \n",
       "3           2014-10-01 09:27:39  2375.0  2014-10-01 09:27:39   192.0   \n",
       "4           2014-10-02 14:34:23   295.0  2014-10-02 14:34:24   295.0   \n",
       "5           2014-05-19 17:50:25  5698.0  2014-05-19 17:50:26  5698.0   \n",
       "\n",
       "                          time9  site10               time10  \n",
       "session_id                                                    \n",
       "1           2014-10-04 12:32:31   309.0  2014-10-04 12:34:31  \n",
       "2           2014-12-02 09:20:42   305.0  2014-12-02 09:20:42  \n",
       "3           2014-10-01 09:27:39     8.0  2014-10-01 09:27:39  \n",
       "4           2014-10-02 14:34:25   295.0  2014-10-02 14:34:26  \n",
       "5           2014-05-19 17:50:27  4173.0  2014-05-19 17:50:27  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46473 entries, 1 to 46473\n",
      "Data columns (total 20 columns):\n",
      "site1     46473 non-null int64\n",
      "time1     46473 non-null object\n",
      "site2     46048 non-null float64\n",
      "time2     46048 non-null object\n",
      "site3     45643 non-null float64\n",
      "time3     45643 non-null object\n",
      "site4     45321 non-null float64\n",
      "time4     45321 non-null object\n",
      "site5     44985 non-null float64\n",
      "time5     44985 non-null object\n",
      "site6     44687 non-null float64\n",
      "time6     44687 non-null object\n",
      "site7     44401 non-null float64\n",
      "time7     44401 non-null object\n",
      "site8     44108 non-null float64\n",
      "time8     44108 non-null object\n",
      "site9     43828 non-null float64\n",
      "time9     43828 non-null object\n",
      "site10    43542 non-null float64\n",
      "time10    43542 non-null object\n",
      "dtypes: float64(9), int64(1), object(10)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**В обучающей выборке – 400 пользователей.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149    4205\n",
       "405    2592\n",
       "984    2399\n",
       "361    2297\n",
       "884    2278\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Пока для прогноза ID пользователя будем использовать только индексы посещенных сайтов. Индексы нумеровались с 1, так что заменим пропуски на нули.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_sites = train_test_df[['site1', 'site2', 'site3', \n",
    "                                     'site4','site5', \n",
    "                                     'site6','site7', 'site8', \n",
    "                                     'site9', 'site10']].fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_test_df_time = train_test_df[['time1', 'time2', 'time3', \n",
    "                                     'time4','time5', \n",
    "                                     'time6','time7', 'time8', \n",
    "                                     'time9', 'time10']].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>site2</th>\n",
       "      <th>site3</th>\n",
       "      <th>site4</th>\n",
       "      <th>site5</th>\n",
       "      <th>site6</th>\n",
       "      <th>site7</th>\n",
       "      <th>site8</th>\n",
       "      <th>site9</th>\n",
       "      <th>site10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23713</td>\n",
       "      <td>23720</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23720</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "      <td>23713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8726</td>\n",
       "      <td>8725</td>\n",
       "      <td>665</td>\n",
       "      <td>8727</td>\n",
       "      <td>45</td>\n",
       "      <td>8725</td>\n",
       "      <td>45</td>\n",
       "      <td>5320</td>\n",
       "      <td>5320</td>\n",
       "      <td>5320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>19</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "      <td>309</td>\n",
       "      <td>303</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1359</td>\n",
       "      <td>925</td>\n",
       "      <td>1240</td>\n",
       "      <td>1360</td>\n",
       "      <td>1344</td>\n",
       "      <td>1359</td>\n",
       "      <td>1346</td>\n",
       "      <td>1345</td>\n",
       "      <td>1344</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>52</td>\n",
       "      <td>85</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>10</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>83</td>\n",
       "      <td>1344</td>\n",
       "      <td>1240</td>\n",
       "      <td>1359</td>\n",
       "      <td>1345</td>\n",
       "      <td>85</td>\n",
       "      <td>1346</td>\n",
       "      <td>83</td>\n",
       "      <td>925</td>\n",
       "      <td>1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13585</td>\n",
       "      <td>13586</td>\n",
       "      <td>13585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29312</td>\n",
       "      <td>29311</td>\n",
       "      <td>29312</td>\n",
       "      <td>29312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>1387</td>\n",
       "      <td>7</td>\n",
       "      <td>72</td>\n",
       "      <td>1386</td>\n",
       "      <td>1384</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "      <td>1563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1  site2  site3  site4  site5  site6  site7  site8  site9  \\\n",
       "session_id                                                                  \n",
       "1           23713  23720  23713  23713  23720  23713  23713  23713  23713   \n",
       "2            8726   8725    665   8727     45   8725     45   5320   5320   \n",
       "3             303     19    303    303    303    303    303    309    303   \n",
       "4            1359    925   1240   1360   1344   1359   1346   1345   1344   \n",
       "5              11     85     52     85     11     52     11     85     10   \n",
       "6              83   1344   1240   1359   1345     85   1346     83    925   \n",
       "7           13585  13585  13585  13585  13585  13585  13585  13585  13586   \n",
       "8           29312  29311  29312  29312      0      0      0      0      0   \n",
       "9              21     62     18   1387      7     72   1386   1384     10   \n",
       "10           1563   1563   1563   1563   1563   1563   1563   1563   1563   \n",
       "\n",
       "            site10  \n",
       "session_id          \n",
       "1            23713  \n",
       "2             5320  \n",
       "3              303  \n",
       "4             1345  \n",
       "5               85  \n",
       "6             1359  \n",
       "7            13585  \n",
       "8                0  \n",
       "9               22  \n",
       "10            1563  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_df_sites.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate additional features\n",
    "def calc_metrics(s, t):\n",
    "    m1, m2, m3, m4, m5, m6 = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    # 1. num of unique sites\n",
    "    m1 = len(np.unique(s))\n",
    "\n",
    "    t_min, t_max = min(t), max(t)\n",
    "            \n",
    "    # 2. start hour\n",
    "    m2 = t_min.hour\n",
    "\n",
    "    # 3. day of week\n",
    "    m3 = t_min.weekday()\n",
    "    \n",
    "    # 4. month\n",
    "    m4 = t_min.month\n",
    "    \n",
    "    # 5. day\n",
    "    m5 = t_min.day\n",
    "    \n",
    "    # 6. day\n",
    "    m6 = t_min.year\n",
    "\n",
    "    return [m1, m2, m3, m4, m5, m6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 229266/229266 [01:54<00:00, 2000.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_test_new_feat = []\n",
    "for ix in tqdm(range(0, train_test_df_time.shape[0])):\n",
    "    s = train_test_df_sites.iloc[ix,:]\n",
    "    t = train_test_df_time.iloc[ix,:]\n",
    "    train_test_new_feat.append(calc_metrics(s,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'new_features_400users.pkl'), 'wb') as new_features_400users_pkl:\n",
    "    pickle.dump(train_test_new_feat, new_features_400users_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'new_features_400users.pkl'), 'rb') as new_features_400users_pkl:\n",
    "    train_test_new_feat = pickle.load(new_features_400users_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 15, 0, 3, 24, 2014],\n",
       " [6, 14, 3, 4, 17, 2014],\n",
       " [3, 10, 4, 3, 21, 2014],\n",
       " [7, 9, 4, 12, 13, 2013],\n",
       " [4, 12, 1, 11, 26, 2013]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_new_feat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Создайте разреженные матрицы *X_train_sparse* и *X_test_sparse* аналогично тому, как мы это делали ранее. Используйте объединенную матрицу *train_test_df_sites*, потом разделите обратно на обучающую и тестовую части.**\n",
    "\n",
    "Обратите внимание на то, что в  сессиях меньше 10 сайтов  у нас остались нули, так что первый признак (сколько раз попался 0) по смыслу отличен от остальных (сколько раз попался сайт с индексом $i$). Поэтому первый столбец разреженной матрицы надо будет удалить.\n",
    "\n",
    "**Выделите в отдельный вектор *y* ответы на обучающей выборке.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_csr(M):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "\n",
    "    # calculate site's number in each session\n",
    "    for row_ix in tqdm(range(0, M.shape[0])):\n",
    "        row = M[row_ix, :]\n",
    "        s_ind, s_count = np.unique(row, return_counts=True)\n",
    "\n",
    "        data.extend(s_count.tolist())\n",
    "        indices.extend(s_ind.tolist())\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    return csr_matrix((data, indices, indptr), dtype=int)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 229266/229266 [00:03<00:00, 57735.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train_test_sparse = convert_to_csr(train_test_df_sites.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sparse = X_train_test_sparse[:train_df.shape[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_sparse = X_train_test_sparse[train_df.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = train_df.user_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_train_400users.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(X_train_sparse, pkl, protocol=2)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_test_400users.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(X_test_sparse, pkl, protocol=2)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_400users.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(y, pkl, protocol=2)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_400users.pkl'), 'wb') as pkl:\n",
    "    pickle.dump(X_train_test_sparse, pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_test_400users.pkl'), 'rb') as pkl:\n",
    "    X_test_sparse = pickle.load(pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_train_400users.pkl'), 'rb') as pkl:\n",
    "    X_train_sparse = pickle.load(pkl)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_400users.pkl'), 'rb') as pkl:\n",
    "    y = pickle.load(pkl)\n",
    "    \n",
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_400users.pkl'), 'rb') as pkl:\n",
    "    X_train_test_sparse = pickle.load(pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Выведите размерности матриц *X_train_sparse* и *X_test_sparse* – 4 числа на одной строке через пробел: число строк и столбцов матрицы *X_train_sparse*, затем число строк и столбцов матрицы *X_test_sparse*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182793 36656 46473 36656\n"
     ]
    }
   ],
   "source": [
    "print(\"{0[0]} {0[1]} {1[0]} {1[1]}\".format(X_train_sparse.shape, X_test_sparse.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сохраним в pickle-файлы объекты *X_train_sparse*, *X_test_sparse* и *y* (последний – в файл *train_target.pkl*).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_train_sparse.pkl'), 'wb') as X_train_sparse_pkl:\n",
    "    pickle.dump(X_train_sparse, X_train_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'X_test_sparse.pkl'), 'wb') as X_test_sparse_pkl:\n",
    "    pickle.dump(X_test_sparse, X_test_sparse_pkl, protocol=2)\n",
    "with open(os.path.join(PATH_TO_DATA, 'train_target.pkl'), 'wb') as train_target_pkl:\n",
    "    pickle.dump(y, train_target_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разобьем обучающую выборку на 2 части в пропорции 7/3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_sparse, y, test_size=0.3, \n",
    "                                                     random_state=17, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Создайте объекты `sklearn.linear_model.SGDClassifier` с логистической функцией потерь и с *hinge loss* (логистическая регрессия и линейный SVM соответственно) и параметром `random_state`=17. Остальные параметры оставьте по умолчанию, разве что `n_jobs`=-1 никогда не помешает. Обучите  модели на выборке `(X_train, y_train)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=17, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_svm = SGDClassifier(loss='hinge', random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=17, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделаем прогнозы с помощью обеих моделей на отложенной выборке *(X_valid, y_valid)*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_valid_pred = sgd_logit.predict(X_valid)\n",
    "svm_valid_pred = sgd_svm.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Напечатайте через пробел доли правильных ответов логистической регрессии и линейного SVM, обученных с помощью стохастического градиентного спуска, на отложенной выборке. Округлите до 3 знаков после разделителя.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.302 0.295\n"
     ]
    }
   ],
   "source": [
    "print(\"%.3f\" % accuracy_score(y_valid, logit_valid_pred), \"%.3f\" % accuracy_score(y_valid, svm_valid_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сделайте прогноз для тестовой выборки с помощью *sgd_logit*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_test_pred = sgd_logit.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Запишите ответы в файл и сделайте посылку на Kaggle. Далее дайте своей команде (из одного человека) на Kaggle говорящее название – по шаблону \"[YDF & MIPT] _Username\", чтоб можно было легко идентифицировать Вашу посылку на [лидерборде](https://inclass.kaggle.com/c/identify-me-if-you-can4/leaderboard).**\n",
    "\n",
    "**Результат, который мы только что получили, соответствует бейзлайну \"SGDCLassifer\" на лидерборде, задача на эту неделю – как минимум его побить, дополнительные баллы будут для тех, кто попадет в топ-10 и топ-30 по итогам этой недели.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='user_id', index_label=\"session_id\"):\n",
    "    # turn predictions into data frame and save as csv file\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_pred, '20170812-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and paramers tuning for SGD, RF, GB, ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1, alpha=1e-5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_svm = SGDClassifier(loss='hinge', random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(sgd_logit, X_train_sparse, y, scoring='accuracy', cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33814784172019224"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['warm_start',\n",
       " 'loss',\n",
       " 'n_jobs',\n",
       " 'eta0',\n",
       " 'verbose',\n",
       " 'shuffle',\n",
       " 'fit_intercept',\n",
       " 'epsilon',\n",
       " 'average',\n",
       " 'n_iter',\n",
       " 'penalty',\n",
       " 'power_t',\n",
       " 'random_state',\n",
       " 'l1_ratio',\n",
       " 'alpha',\n",
       " 'learning_rate',\n",
       " 'class_weight']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_svm.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'alpha' : [0.000001, 0.00001, 0.0001],\n",
    "    #'eta0' : [0, 0.001, 0.05],\n",
    "    'n_iter' : [5, 10, 15],\n",
    "    'power_t' : [0.2, 0.5, 1, 2] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(SGDClassifier(loss='log', random_state=17, n_jobs=-1), parameters_grid, scoring = 'accuracy', cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11h 48min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=17, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=17, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [1e-06, 1e-05, 0.0001], 'power_t': [0.2, 0.5, 1, 2], 'n_iter': [5, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.26078, std: 0.00324, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.26078, std: 0.00324, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.26078, std: 0.00324, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.26078, std: 0.00324, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.28139, std: 0.00472, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.28139, std: 0.00472, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.28139, std: 0.00472, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.28139, std: 0.00472, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.29318, std: 0.00293, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.29318, std: 0.00293, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.29318, std: 0.00293, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.29318, std: 0.00293, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 15},\n",
       " mean: 0.30559, std: 0.00215, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.30559, std: 0.00215, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.30559, std: 0.00215, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.30559, std: 0.00215, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.32705, std: 0.00466, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.32705, std: 0.00466, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.32705, std: 0.00466, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.32705, std: 0.00466, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.33815, std: 0.00107, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.33815, std: 0.00107, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.33815, std: 0.00107, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.33815, std: 0.00107, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 15},\n",
       " mean: 0.29920, std: 0.00096, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.29920, std: 0.00096, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.29920, std: 0.00096, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.29920, std: 0.00096, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.30661, std: 0.00041, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.30661, std: 0.00041, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.30661, std: 0.00041, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.30661, std: 0.00041, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.30775, std: 0.00052, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.30775, std: 0.00052, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.30775, std: 0.00052, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.30775, std: 0.00052, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 15}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(SGDClassifier(loss='hinge', random_state=17, n_jobs=-1), parameters_grid, scoring = 'accuracy', cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'alpha' : [0.000001, 0.00001, 0.0001],\n",
    "    'eta0' : [0, 0.001, 0.05],\n",
    "    'n_iter' : [5, 10, 15]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9h 39min 26s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=17, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=17, shuffle=True, verbose=0,\n",
       "       warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [1e-06, 1e-05, 0.0001], 'power_t': [0.2, 0.5, 1, 2], 'n_iter': [5, 10, 15]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.25471, std: 0.00188, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.25471, std: 0.00188, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.25471, std: 0.00188, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.25471, std: 0.00188, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.27360, std: 0.00257, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.27360, std: 0.00257, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.27360, std: 0.00257, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.27360, std: 0.00257, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.28524, std: 0.00059, params: {'alpha': 1e-06, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.28524, std: 0.00059, params: {'alpha': 1e-06, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.28524, std: 0.00059, params: {'alpha': 1e-06, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.28524, std: 0.00059, params: {'alpha': 1e-06, 'power_t': 2, 'n_iter': 15},\n",
       " mean: 0.27433, std: 0.00296, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.27433, std: 0.00296, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.27433, std: 0.00296, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.27433, std: 0.00296, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.29606, std: 0.00524, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.29606, std: 0.00524, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.29606, std: 0.00524, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.29606, std: 0.00524, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.30897, std: 0.00190, params: {'alpha': 1e-05, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.30897, std: 0.00190, params: {'alpha': 1e-05, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.30897, std: 0.00190, params: {'alpha': 1e-05, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.30897, std: 0.00190, params: {'alpha': 1e-05, 'power_t': 2, 'n_iter': 15},\n",
       " mean: 0.29776, std: 0.00074, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 5},\n",
       " mean: 0.29776, std: 0.00074, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 5},\n",
       " mean: 0.29776, std: 0.00074, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 5},\n",
       " mean: 0.29776, std: 0.00074, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 5},\n",
       " mean: 0.31064, std: 0.00344, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 10},\n",
       " mean: 0.31064, std: 0.00344, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 10},\n",
       " mean: 0.31064, std: 0.00344, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 10},\n",
       " mean: 0.31064, std: 0.00344, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 10},\n",
       " mean: 0.31990, std: 0.00245, params: {'alpha': 0.0001, 'power_t': 0.2, 'n_iter': 15},\n",
       " mean: 0.31990, std: 0.00245, params: {'alpha': 0.0001, 'power_t': 0.5, 'n_iter': 15},\n",
       " mean: 0.31990, std: 0.00245, params: {'alpha': 0.0001, 'power_t': 1, 'n_iter': 15},\n",
       " mean: 0.31990, std: 0.00245, params: {'alpha': 0.0001, 'power_t': 2, 'n_iter': 15}]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=17, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['warm_start',\n",
       " 'oob_score',\n",
       " 'n_jobs',\n",
       " 'verbose',\n",
       " 'max_leaf_nodes',\n",
       " 'bootstrap',\n",
       " 'min_samples_leaf',\n",
       " 'n_estimators',\n",
       " 'min_samples_split',\n",
       " 'min_weight_fraction_leaf',\n",
       " 'criterion',\n",
       " 'random_state',\n",
       " 'min_impurity_split',\n",
       " 'max_features',\n",
       " 'max_depth',\n",
       " 'class_weight']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters_grid = {\n",
    "    'n_estimators' : [10, 20, 30],\n",
    "    'bootstrap' : [True, False],\n",
    "    'max_depth' : [10, 20, 30],\n",
    "    'max_features' : ['sqrt', 'log2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(RandomForestClassifier(random_state=17, n_jobs=-1), parameters_grid, scoring = 'accuracy', cv = skf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7h 10min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=3, random_state=17, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=17,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 20, 30], 'max_features': ['sqrt', 'log2'], 'bootstrap': [True, False], 'max_depth': [10, 20, 30]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "grid_cv.fit(X_train_sparse, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.06949, std: 0.00143, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.08659, std: 0.00246, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.09666, std: 0.00134, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.03764, std: 0.00298, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.04791, std: 0.00326, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.05166, std: 0.00188, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 10},\n",
       " mean: 0.10004, std: 0.00155, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.12322, std: 0.00073, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.13139, std: 0.00018, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.05212, std: 0.00468, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.06756, std: 0.00265, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.07941, std: 0.00196, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 20},\n",
       " mean: 0.12510, std: 0.00192, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.14807, std: 0.00092, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.15711, std: 0.00124, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.06276, std: 0.00420, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.08720, std: 0.00186, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.10286, std: 0.00220, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': True, 'max_depth': 30},\n",
       " mean: 0.07149, std: 0.00238, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.08871, std: 0.00244, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.09709, std: 0.00129, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.03867, std: 0.00288, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.04857, std: 0.00359, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.05559, std: 0.00219, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 10},\n",
       " mean: 0.10185, std: 0.00328, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.12203, std: 0.00253, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.13110, std: 0.00298, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.05270, std: 0.00292, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.06961, std: 0.00462, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.08378, std: 0.00130, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 20},\n",
       " mean: 0.12508, std: 0.00388, params: {'max_features': 'sqrt', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 30},\n",
       " mean: 0.14615, std: 0.00279, params: {'max_features': 'sqrt', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 30},\n",
       " mean: 0.15717, std: 0.00234, params: {'max_features': 'sqrt', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 30},\n",
       " mean: 0.06377, std: 0.00308, params: {'max_features': 'log2', 'n_estimators': 10, 'bootstrap': False, 'max_depth': 30},\n",
       " mean: 0.08856, std: 0.00337, params: {'max_features': 'log2', 'n_estimators': 20, 'bootstrap': False, 'max_depth': 30},\n",
       " mean: 0.10851, std: 0.00118, params: {'max_features': 'log2', 'n_estimators': 30, 'bootstrap': False, 'max_depth': 30}]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb = xgboost.XGBClassifier(n_estimators=10, n_jobs=-1, silent=False)\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X_train_sparse, y, test_size=0.3, random_state=9, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=10,\n",
       "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=False, subsample=1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb.fit(X_trn, y_trn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19222072285641342"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_tst, xgb.predict(X_tst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb = lightgbm.LGBMClassifier(n_estimators=20, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 47s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=10,\n",
       "        min_child_weight=5, min_split_gain=0, n_estimators=20, nthread=-1,\n",
       "        num_leaves=31, objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        seed=0, silent=False, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgb.fit(X_trn.astype(float), y_trn.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17847113315584084"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_tst, lgb.predict(X_tst.astype(float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пробуем добавить признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "def prediction_bag(predictions, weights=\"uniform\"):\n",
    "    voting = []\n",
    "    scores = defaultdict(list)\n",
    "    \n",
    "    weight_list = [1] * len(predictions)\n",
    "    for i, prediction in enumerate(predictions):        \n",
    "        if weights != \"uniform\":            \n",
    "            weight_list[i] = weight_list[i] * weights[i]\n",
    "        \n",
    "        for ix, val in enumerate(prediction):\n",
    "            scores[ix].extend(weight_list[i] * [val])\n",
    "        \n",
    "    for ix in sorted(scores):\n",
    "        voting.append(Counter(scores[ix]).most_common(1)[0][0])\n",
    "    \n",
    "    return voting\n",
    "\n",
    "def models_assessment(classifiers, X, y, test_size=0.3):\n",
    "    cls_predictions = []\n",
    "    X_trn, X_tst, y_trn, y_tst = X, X, y, y\n",
    "    if test_size > 0:\n",
    "        X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=test_size, random_state=17, stratify=y)\n",
    "    \n",
    "    for cls_estimator in classifiers:\n",
    "        cls_name = type(cls_estimator)\n",
    "        start_time = timeit.default_timer()\n",
    "        cls_estimator.fit(X_trn, y_trn)\n",
    "        y_cls = cls_estimator.predict(X_tst)\n",
    "        cls_predictions.append(y_cls)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(cls_name, \"%.3f\" % accuracy_score(y_tst, y_cls), elapsed)\n",
    "    \n",
    "    return cls_predictions, y_tst\n",
    "\n",
    "def make_predictions(classifiers, X):\n",
    "    cls_predictions = []\n",
    "    for cls_estimator in classifiers:\n",
    "        cls_name = type(cls_estimator)\n",
    "        start_time = timeit.default_timer()        \n",
    "        y_cls = cls_estimator.predict(X)\n",
    "        cls_predictions.append(y_cls)\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(cls_name, elapsed)\n",
    "    \n",
    "    return cls_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_NF = np.array(train_test_new_feat)[:X_train_sparse.shape[0], :]\n",
    "X_test_NF = np.array(train_test_new_feat)[X_train_sparse.shape[0]:, :]\n",
    "\n",
    "# TRAIN DATASET\n",
    "encoder = OneHotEncoder()\n",
    "X_NFCat = encoder.fit_transform(X_train_NF[:,2:])\n",
    "X_NFNonCat = X_train_NF[:,:2]\n",
    "\n",
    "X_train_sparseNF = scipy.sparse.hstack([X_train_sparse, X_NFNonCat, X_NFCat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=3, random_state=17)\n",
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1, alpha=1e-5, n_iter=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "scores = cross_val_score(sgd_logit, X_train_sparseNF, y, \n",
    "                         scoring='accuracy', cv=skf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35661514631470731"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=1e-05, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', n_iter=15, n_jobs=-1,\n",
       "       penalty='l2', power_t=0.5, random_state=17, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sgd_logit.fit(X_train_sparseNF, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare test dataset\n",
    "# TEST DATASET\n",
    "X_NFNonCat = X_test_NF[:,:2]\n",
    "X_NFCat = encoder.transform(X_test_NF[:,2:])\n",
    "\n",
    "X_test_sparseNF = scipy.sparse.hstack([X_test_sparse, X_NFNonCat, X_NFCat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 621 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "logit_test_predNF = sgd_logit.predict(X_test_sparseNF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(logit_test_predNF, '20170819-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying ensembles with basic classifiers from week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=25, random_state=17, n_jobs=-1)\n",
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1, , alpha=1e-5, n_iter=15)\n",
    "sgd_svm = SGDClassifier(loss='hinge', random_state=17, n_jobs=-1, alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train150, X_test150, y_train150, y_test150 = train_test_split(X_150sparse, y_150, test_size=0.3, \n",
    "                                                                random_state=17, stratify=y_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers=[('log', sgd_logit), ('svm', sgd_svm), ('mod_hub', sgd_hub), ('shinge', sgd_shin), ('perc', sgd_perc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 0.426 6.03260430769\n",
      "svm 0.420 5.062624\n",
      "mod_hub 0.415 5.32296984615\n",
      "shinge 0.396 5.27191876923\n",
      "perc 0.390 5.11173415384\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_predictions = models_assessment(classifiers, X_150sparse, y_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_vote = prediction_bag(cls_predictions, weights=[3,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.441\n"
     ]
    }
   ],
   "source": [
    "print(\"%.3f\" % accuracy_score(y_test150, y_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 0.302 19.2451688205\n",
      "svm 0.295 15.9992172308\n",
      "mod_hub 0.289 16.9515798974\n",
      "shinge 0.203 15.8908988718\n",
      "perc 0.255 15.7406711795\n",
      "forest 0.315 864.801350154\n",
      "Wall time: 15min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# now let's make another submission\n",
    "classifiers=[('log', sgd_logit), ('svm', sgd_svm), ('mod_hub', sgd_hub), ('shinge', sgd_shin), ('perc', sgd_perc),\n",
    "            ('forest', forest)]\n",
    "cls_predictions = []\n",
    "for cls_name, cls_estimator in classifiers:\n",
    "    start_time = timeit.default_timer()\n",
    "    cls_estimator.fit(X_train, y_train)\n",
    "    y_cls = cls_estimator.predict(X_valid)\n",
    "    cls_predictions.append(y_cls)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(cls_name, \"%.3f\" % accuracy_score(y_valid, y_cls), elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333\n"
     ]
    }
   ],
   "source": [
    "y_vote = prediction_bag(cls_predictions, weights=[2,1,1,1,1,3])\n",
    "print(\"%.3f\" % accuracy_score(y_valid, y_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log 1.46372389744\n",
      "svm 0.502006564104\n",
      "mod_hub 0.510998153848\n",
      "shinge 0.460102974357\n",
      "perc 0.456911999998\n",
      "forest 14.1221083077\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# make predictions\n",
    "classifiers=[('log', sgd_logit), ('svm', sgd_svm), ('mod_hub', sgd_hub), ('shinge', sgd_shin), ('perc', sgd_perc),\n",
    "            ('forest', forest)]\n",
    "cls_predictions = []\n",
    "for cls_name, cls_estimator in classifiers:\n",
    "    start_time = timeit.default_timer()\n",
    "    y_cls = cls_estimator.predict(X_test_sparse)\n",
    "    cls_predictions.append(y_cls)\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(cls_name, elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_vote = prediction_bag(cls_predictions, weights=[2,1,1,1,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(np.array(y_vote), '20170815-01.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017-08-23 submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1, alpha=1e-5, n_iter=15)\n",
    "sgd_svm = SGDClassifier(loss='hinge', random_state=17, n_jobs=-1, alpha=1e-4, n_iter=15)\n",
    "sgd_hub = SGDClassifier(loss='modified_huber', random_state=17, n_jobs=-1)\n",
    "xgb = xgboost.XGBClassifier(n_estimators=15, n_jobs=-1, silent=False)\n",
    "lgb = lightgbm.LGBMClassifier(n_estimators=50, silent=False, nthread=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers=[sgd_logit, sgd_svm, sgd_hub, xgb, lgb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.439 86.4972931304\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.409 72.3206147113\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.400 24.7632088097\n",
      "<class 'xgboost.sklearn.XGBClassifier'> 0.243 2712.53635525\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'> 0.281 961.209285982\n",
      "Wall time: 1h 4min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_predictions, y_verify = models_assessment(classifiers, X_train_sparse.astype(float), y.astype(float), test_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.452\n"
     ]
    }
   ],
   "source": [
    "y_vote = prediction_bag(cls_predictions, weights=[1,1,1,1,1])\n",
    "print(\"%.3f\" % accuracy_score(y_verify, y_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.570333480748\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.495589336955\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.479659093617\n",
      "<class 'xgboost.sklearn.XGBClassifier'> 28.7180703852\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'> 72.4175323754\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_predictions2 = make_predictions(classifiers, X_test_sparse.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_vote = prediction_bag(cls_predictions2, weights=[1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_submission_file(np.array(y_vote, dtype=int), '20170822-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions_blend(clfs, X, y, X_submission, n_folds=3, n_classes=400):\n",
    "    skf = list(StratifiedKFold(n_folds).split(X, y))\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs) * n_classes))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs) * n_classes))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print(\"Classifier\", j, type(clf))\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0],  n_classes))\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print(\"Fold\", i)\n",
    "            X_train = X[train]\n",
    "            y_train = y[train]\n",
    "            X_test = X[test]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)\n",
    "            dataset_blend_train[test, j*n_classes:(j+1)*n_classes] = y_submission\n",
    "            dataset_blend_test_j = dataset_blend_test_j + (1./n_folds) * clf.predict_proba(X_submission)\n",
    "        \n",
    "        dataset_blend_test[:, j*n_classes:(j+1)*n_classes] = dataset_blend_test_j\n",
    "\n",
    "    print()\n",
    "    print(\"Blending...\")\n",
    "    clf = SGDClassifier(n_jobs=-1)\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    y_submission = clf.predict(dataset_blend_test)\n",
    "    \n",
    "    return y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_predictions_blend2(clfs, X, y, X_submission, n_folds=3, n_classes=400):\n",
    "    skf = list(StratifiedKFold(n_folds).split(X, y))\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs) * n_classes))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs) * n_classes))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print(\"Classifier\", j, type(clf), datetime.now())\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0],  n_classes))\n",
    "        \n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print(\"Fold\", i, datetime.now())\n",
    "            X_train = X[train]\n",
    "            y_train = y[train]\n",
    "            X_test = X[test]\n",
    "            y_test = y[test]\n",
    "            \n",
    "            clf.fit(X_train, y_train)\n",
    "            y_submission = clf.predict_proba(X_test)\n",
    "            dataset_blend_train[test, j*n_classes:(j+1)*n_classes] = y_submission\n",
    "            dataset_blend_test_j = dataset_blend_test_j + (1./n_folds) * clf.predict_proba(X_submission)\n",
    "        \n",
    "        dataset_blend_test[:, j*n_classes:(j+1)*n_classes] = dataset_blend_test_j\n",
    "\n",
    "    print()\n",
    "    print(\"Blending...\", datetime.now())\n",
    "    clf = LogisticRegression(n_jobs=4)\n",
    "    clf.fit(dataset_blend_train, y)\n",
    "    y_submission = clf.predict(dataset_blend_test)\n",
    "    \n",
    "    return y_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0 <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 2017-08-23 10:23:11.013000\n",
      "Fold 0 2017-08-23 10:23:11.026000\n",
      "Fold 1 2017-08-23 10:24:09.373000\n",
      "Fold 2 2017-08-23 10:25:07.244000\n",
      "Classifier 1 <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 2017-08-23 10:26:04.172000\n",
      "Fold 0 2017-08-23 10:26:04.202000\n",
      "Fold 1 2017-08-23 10:26:49.232000\n",
      "Fold 2 2017-08-23 10:27:34.587000\n",
      "Classifier 2 <class 'lightgbm.sklearn.LGBMClassifier'> 2017-08-23 10:28:19.281000\n",
      "Fold 0 2017-08-23 10:28:19.319000\n",
      "Fold 1 2017-08-23 10:39:05.794000\n",
      "Fold 2 2017-08-23 10:50:02.562000\n",
      "Classifier 3 <class 'sklearn.ensemble.forest.RandomForestClassifier'> 2017-08-23 11:00:45.415000\n",
      "Fold 0 2017-08-23 11:00:45.458000\n",
      "Fold 1 2017-08-23 11:06:13.603000\n",
      "Fold 2 2017-08-23 11:11:27.837000\n",
      "\n",
      "Blending... 2017-08-23 11:17:08.378000\n",
      "Wall time: 5h 10min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_blend = make_predictions_blend2([sgd_logit, sgd_hub, lgb, forest_g], \n",
    "                                 X_train_sparse.astype(float), np.array(y).astype(float), \n",
    "                                 X_test_sparse.astype(float), n_classes=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(np.array(y_blend.astype(int)), '20170823-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier 0 <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Classifier 1 <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'>\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "Classifier 2 <class 'lightgbm.sklearn.LGBMClassifier'>\n",
      "Fold 0\n",
      "Fold 1\n",
      "Fold 2\n",
      "\n",
      "Blending...\n",
      "Wall time: 45min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_blend = make_predictions_blend2([sgd_logit, sgd_hub, lgb], X_150sparse.astype(float), np.array(y_150).astype(float), \n",
    "                                 X_150sparse.astype(float), n_classes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510\n"
     ]
    }
   ],
   "source": [
    "print(\"%.3f\" % accuracy_score(y_150.astype(float), y_blend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying ensembles with new features for 150 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATA, 'X_sparse_150users.pkl'), 'rb') as Sparse150users_pkl:\n",
    "    X_150sparse = pickle.load(Sparse150users_pkl)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATA, 'y_150users.pkl'), 'rb') as y150users_pkl:\n",
    "    y_150 = pickle.load(y150users_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_logit = SGDClassifier(loss='log', random_state=17, n_jobs=-1, alpha=1e-5, n_iter=15)\n",
    "sgd_svm = SGDClassifier(loss='hinge', random_state=17, n_jobs=-1, alpha=1e-4, n_iter=15)\n",
    "sgd_hub = SGDClassifier(loss='modified_huber', random_state=17, n_jobs=-1, n_iter=15)\n",
    "xgb = xgboost.XGBClassifier(n_estimators=15, n_jobs=-1, silent=False, max_depth=6)\n",
    "lgb = lightgbm.LGBMClassifier(n_estimators=50, silent=False, nthread=4)\n",
    "forest_g = RandomForestClassifier(n_estimators=20, n_jobs=-1, criterion='gini')\n",
    "forest_en = RandomForestClassifier(n_estimators=15, n_jobs=-1, criterion='entropy')\n",
    "extree_g = ExtraTreesClassifier(n_estimators=15, n_jobs=-1, criterion='gini')\n",
    "extree_en = ExtraTreesClassifier(n_estimators=15, n_jobs=-1, criterion='entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifiers=[sgd_logit, sgd_hub, lgb, forest_g]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.444 16.2292773503\n",
      "<class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> 0.437 13.7105602375\n",
      "<class 'lightgbm.sklearn.LGBMClassifier'> 0.351 356.914311865\n",
      "<class 'sklearn.ensemble.forest.RandomForestClassifier'> 0.393 129.855878682\n",
      "Wall time: 8min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_predictions, y_verify = models_assessment(classifiers, X_150sparse.astype(float), y_150.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.461\n"
     ]
    }
   ],
   "source": [
    "y_vote = prediction_bag(cls_predictions, weights=[1,1,1,1])\n",
    "print(\"%.3f\" % accuracy_score(y_verify, y_vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.66901575  0.55994654  0.49936139  0.57015764  0.50373599\n",
      "   0.58402801  0.53919013]\n",
      " [ 0.66901575  1.          0.52737125  0.47089862  0.54017446  0.47493743\n",
      "   0.55401939  0.51538461]\n",
      " [ 0.55994654  0.52737125  1.          0.65907508  0.59093243  0.54859315\n",
      "   0.58538723  0.56251848]\n",
      " [ 0.49936139  0.47089862  0.65907508  1.          0.54374809  0.51405253\n",
      "   0.53672738  0.52286698]\n",
      " [ 0.57015764  0.54017446  0.59093243  0.54374809  1.          0.63093381\n",
      "   0.69889343  0.66061194]\n",
      " [ 0.50373599  0.47493743  0.54859315  0.51405253  0.63093381  1.\n",
      "   0.62535538  0.63835554]\n",
      " [ 0.58402801  0.55401939  0.58538723  0.53672738  0.69889343  0.62535538\n",
      "   1.          0.68671016]\n",
      " [ 0.53919013  0.51538461  0.56251848  0.52286698  0.66061194  0.63835554\n",
      "   0.68671016  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.corrcoef(cls_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adaboost 0.453 450.717513026\n",
      "Wall time: 7min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_predictions, y_verify = models_assessment([('adaboost', AdaBoostClassifier(\n",
    "    base_estimator=SGDClassifier(loss='log', random_state=17, n_jobs=-1), random_state=17))], X_150sparseNFScaled, y_150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пользователь в Kaggle - [YDF & MIPT] _VladNF Public score: 0.20395 (3rd result at 3-45 pm Aug-23)\n",
    "\n",
    "1. Среди базовых классификаторов наилучшее качество на тестовой выборке показал SGDClassifier, после оптимизации параметров - позволяет преодолеть бейзлайн.\n",
    "2. Несмотря на то, что feature engineering давал существенное улучшение на train, на тесте результаты были существенно хуже. В итоге лучший результат был получен без генерации новых признаков\n",
    "3. Смешивание алгоритмов - алгоритм голосования, один из самых простых - позволил улучшить качество. Наилучший результат был получен с помощью приема blending из статьи по ссылке - код реализации make_predictions_blend2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерии оценки работы:\n",
    "-  Правильные ли получились размерности матриц в п. 1? (max. 2 балла)\n",
    "-  Правильные ли получились доли правильных ответов логистической регрессии и линейного SVM в п . 2? (max. 4 балла)\n",
    "-  Каков результат лучшей из посылок на Kaggle, побит ли бейзлайн SGDCLassifier?  (max. 2 балла)\n",
    "-  Входит ли  посылка в топ-30 лучших на данный момент на публичном лидерборде соревнования? (max. 2 балла)\n",
    "-  Входит ли  посылка в топ-10 лучших на данный момент на публичном лидерборде соревнования? (max. 4 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Пути улучшения\n",
    "На этой неделе дается много времени на соревнование. Не забывайте вносить хорошие идеи, к которым Вы пришли по ходу соревнования, в описание финального проекта (`html`, `pdf` или `ipynb`).\n",
    "Что можно попробовать:\n",
    " - Использовать ранее построенные признаки для улучшения модели (проверить их можно на меньшей выборке по 150 пользователям – это быстрее)\n",
    " - Настроить параметры моделей (например, коэффициенты регуляризации)\n",
    " - Если позволяют мощности (или хватает терпения), можно попробовать смешивание (блендинг) ответов бустинга и линейной модели. [Вот](http://mlwave.com/kaggle-ensembling-guide/) один из самых известных тьюториалов по смешиванию ответов алгоритмов\n",
    " - Обратите внимание, что в соревновании также даны исходные данные о посещенных 400 пользователями веб-страницах (400 csv-файлов в *train.zip*). По этим данным можно сформировать свою обучающую выборку. \n",
    "\n",
    "На 6 неделе мы пройдем большой тьюториал по Vowpal Wabbit и попробуем его в деле, на данных соревнования."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
