{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо ввести следующие обозначения:\n",
    "- $x_{ij}$ — значение признака j на объекте i,\n",
    "- $\\bar{x}_j$ — среднее значение признака j по всей выборке,\n",
    "- $y_i$ — значение целевой переменной или ответа на объекте i,\n",
    "- $\\bar{y}$ — среднее значение целевой переменной на всей выборке.\n",
    "\n",
    "Задача — оценить предсказательную силу (информативность) каждого признака, то есть насколько хорошо по данному признаку можно предсказывать целевую переменную. Данные оцененной информативности\n",
    "можно использовать, чтобы отобрать k лучших признаков или признаки, у которых значение информативности больше порога (например, некоторой квантили распределения информативности)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Одномерный отбор признаков:\n",
    "\n",
    "#### Отбор признаков с использованием корреляции\n",
    "- По типу данных **(признак, ответы): (бинарный, бинарный)** -> корреляция Пирсона\n",
    "\n",
    "#### Использование бинарного классификатора для отбора признаков\n",
    "- построение классификатора по каждому признаку. Например, можно рассмотреть очень простой классификатор,\n",
    "который берёт значение признака j на объекте, сравнивает его с порогом t, и если значение меньше этого\n",
    "порога, то он относит объект к первому классу, если же меньше порога — то к другому, нулевому или\n",
    "минус первому, в зависимости от того, как мы его обозначили. Далее, поскольку этот классификатор зависит\n",
    "от порога t, то его качество можно измерить с помощью таких метрик как площадь под ROC-кривой или\n",
    "Precision-Recall кривой, а затем по данной площади отсортировать все признаки и выбирать лучшие.\n",
    "\n",
    "#### Использование метрик теории информации для отбора признаков\n",
    "- взаимная информация, или mutual\n",
    "information. Она рассчитана на случай, когда **и признак, и целевая переменная — дискретные**.\n",
    "Пусть необходимо решить задачу многоклассовой классификации. В этом случае целевая переменная\n",
    "принимает m различных значений:\n",
    "1, 2, . . . , m,\n",
    "а признак — n значений:\n",
    "1, 2, ..., n.\n",
    "Поскольку для метрики взаимной информации не важны конкретные значения признаков, можно обозначать\n",
    "их с помощью натуральных чисел. Вероятностью некоторого события будем обозначать долю объектов, для\n",
    "которых это событие выполнено. Например, вероятность того, что признак принимает значение v, а целевая\n",
    "переменная — k, вычисляется следующим образом:\n",
    "$$P(x_j=v, y=k)=\\frac{1}{l}\\sum_{i=1}^{l}[x_{ij} = v][y_i = k]$$\n",
    "Или, например, вероятность того, что признак равен v:\n",
    "$$P(x_j = v) =\\sum_{i=1}^{n}[x_{ij}=v]$$\n",
    "Взаимная информация между признаком j и целевой переменой вычисляется по следующей формуле:\n",
    "$$MI_j =\\sum_{v=1}^{n}\\sum_{k=1}^{m}P(x_j = v,y=k)log\\frac {P (x_j = v, y = k)}{P (x_j = v)P (y = k)}$$\n",
    "Главная особенность взаимной информации состоит в следующем: она равна нулю, если признак и целевая\n",
    "переменная независимы. Если же между ними есть какая-то связь, то взаимная информация будет отличаться от нуля, причём она может быть как больше, так и меньше нуля. Это означает, что информативность\n",
    "признаков нужно оценивать по модулю взаимной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Жадные методы отбора признаков\n",
    "\n",
    "Жадные методы отбора признаков, по сути своей, являются надстройками над методами обучения моделей.\n",
    "Они перебирают различные подмножества признаков и выбирают то из них, которое дает наилучшее качество\n",
    "определённой модели машинного обучения.\n",
    "Данный процесс устроен следующим образом. Обучение модели считается черным ящиком, который на\n",
    "вход принимает информацию о том, какие из его признаков можно использовать при обучении модели, обучает модель, и дальше каким-то методом оценивается качество такой модели, например, по отложенной выборке\n",
    "или кросс-валидации. Таким образом, задача, которую необходимо решить, — это оптимизация функционала\n",
    "качества модели по подмножеству признаков\n",
    "\n",
    "#### Переборные методы\n",
    "Это подход очень хороший,он найдет оптимальное подмножество признаков, но при этом очень сложный, поскольку всего таких подмножеств $2^d$ , где d — число признаков\n",
    "\n",
    "#### Метод жадного добавления\n",
    "\n",
    "Жадная стратегия используется всегда, когда полный перебор не подходит для решения задачи. Например, может оказаться неплохой стратегия жадного наращивания (жадного добавления). Сначала находится\n",
    "один признак, который дает наилучшее качество модели (наименьшую ошибку Q):\n",
    "$$i_1 = argmin Q(i)$$.\n",
    "Тогда множество, состоящее из этого признака:\n",
    "$$J_1 = \\{i_1\\}$$\n",
    "Дальше к этому множеству добавляется еще один признак так, чтобы как можно сильнее уменьшить ошибку\n",
    "модели:\n",
    "$$i_2 = argmin Q(i_1 ,i), J_2 = \\{i_1 , i_2\\}.$$\n",
    "Далее каждый раз добавляется по одному признаку, образуются множества $J_3$ , $J_4$ , ... Если в какой-то момент\n",
    "невозможно добавить новый признак так, чтобы уменьшить ошибку, процедура останавливается. Жадность\n",
    "процедуры заключается в том, что как только какой-то признак попадает в оптимальное множество, его\n",
    "нельзя оттуда удалить.\n",
    "\n",
    "#### Алгоритм ADD-DEL\n",
    "Алгоритм начинается с процедуры жадного добавления. Множество признаков\n",
    "наращивается до тех пор, пока получается уменьшить ошибку, затем признаки жадно удаляются из подмно-\n",
    "жества, то есть перебираются все возможные варианты удаления признака, оценивается ошибка и удаляется\n",
    "тот признак, который приводит к наибольшему уменьшению ошибки на выборке. Эта процедура повторяет\n",
    "добавление и удаление признаков до тех пор, пока уменьшается ошибка. Алгоритм ADD-DEL всё еще жадный, но при этом он менее жадный, чем предыдущий, поскольку может исправлять ошибки, сделанные в\n",
    "начале перебора: если вначале был добавлен неинформативный признак, то на этапе удаления от него можно\n",
    "избавиться."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор признаков на основе моделей\n",
    "\n",
    "#### Использование линейных моделей для отбора признаков\n",
    "\n",
    "Линейная модель вычисляет взвешенную сумму значений признаков на объекте:\n",
    "$$a(x) =\\sum_{j=1}^{d}w_jx^j$$\n",
    "При этом возвращается сама взвешенная сумма, если это задача регрессии, и знак этой суммы, если это\n",
    "задача классификации. Если признаки масштабированы, то веса при признаках можно интерпретировать\n",
    "как информативности: чем больше по модулю вес при признаке j, тем больший вклад этот признак вносит\n",
    "в ответ модели.\n",
    "\n",
    "Если необходимо обнулить как можно больше весов, чтобы линейная модель учитывала только те признаки, которые наиболее важны для нее, можно использовать L1-регуляризацию. Чем больше коэффициент\n",
    "при L1-регуляризаторе, тем меньше признаков будет использовать линейная модель.\n",
    "\n",
    "#### Применение решающих деревьев для отбора признаков\n",
    "\n",
    "Еще один вид моделей, обсуждаемых в предыдущем курсе, — это решающие деревья. Решающие деревья\n",
    "строятся «жадно»: они растут от корня к листьям, и на каждом этапе происходит попытка разбить вершину\n",
    "на две. Чтобы разбить вершину, нужно выбрать признак, по которому будет происходить разбиение, и порог,\n",
    "с которым будет сравниваться значение данного признака. Если значение признака меньше этого порога,\n",
    "то объект отправляется в левое поддерево, если больше — в правое поддерево. Выбор признака и порога\n",
    "осуществляется по следующему критерию:\n",
    "$$Q(X_m , j, t) =\\frac{|X_l|}{|X_m|}H(X_l)+\\frac{|X_r|}{|X_m|}H(X_r) → min_{j,t}$$\n",
    "где $H(X)$ — это критерий информативности. Ранее в курсе были рассмотрены различные критерии информативности. Например, в задаче регрессии используется функционал среднеквадратичной ошибки, а в классификации — критерий Джини или энтропийный критерий.\n",
    "\n",
    "Критерий Q вычисляет взвешенную сумму критериев информативности H(X) в обеих дочерних вершинах\n",
    "— левой и правой. Чем меньше данная взвешенная сумма, тем больше признак j и порог t подходят для\n",
    "разбиения.\n",
    "\n",
    "Если в данной вершине происходит разбиение по признаку j, то чем сильнее уменьшается значение кри-\n",
    "терия информативности, тем важнее этот признак оказался при построении дерева. Таким образом, можно\n",
    "оценивать важность признака на основе того, насколько сильно он смог уменьшить значение критерия инфор-\n",
    "мативности. Пусть, в вершине m произведено разбиение по признаку j. Тогда можно вычислить уменьшение\n",
    "критерия информативности в ней по следующей формуле:\n",
    "$$H(X_m)−\\frac{|X_l|}{|X_m|}H(X_l)−\\frac{|X_r|}{|X_m|}H(X_r)$$\n",
    "$R_j$ - сумма данного уменьшения по всем вершинам дерева, в которых происходило разбиение по признаку j.\n",
    "Чем больше $R_j$ , тем важнее данный признак был при построении дерева.\n",
    "\n",
    "#### Использование композиций алгоритмов для отбора признаков\n",
    "Сами по себе решающие деревья не очень полезны, но они очень активно используются при построении ком-\n",
    "позиций, в частности, в случайных лесах и в градиентном бустинге над деревьями. В данных композициях\n",
    "измерить важность признака можно аналогичным образом: суммируется уменьшение критерия информатив-\n",
    "ности $R_j$ по всем деревьям композиции, и чем больше данная сумма, тем важнее признак $j$ для композиции.\n",
    "То есть признаки оцениваются с помощью того, насколько сильно они смогли уменьшить значение критерия\n",
    "информативности в совокупности по всем деревьям композиции.\n",
    "Для случайного леса можно предложить еще один интересный способ оценивания информативности при-\n",
    "знаков. В этой композиции каждое базовое дерево $b_n$ обучается по подмножеству объектов обучающей вы-\n",
    "борки. Таким образом, есть объекты, на которых дерево не обучалось, и набор этих объектов является вали-\n",
    "дационной выборкой для дерева n. Такая выборка называется out-of-bag. Итак, метод заключается в следу-\n",
    "ющем: ошибку $Q_n$ базового дерева $b_n$ оценивают по out-of-bag-выборке и запоминают. После этого признак\n",
    "$j$ превращают в абсолютно бесполезный, шумовой: в матрицу «объекты-признаки» все значения в столбце\n",
    "$j$ перемешивают. Затем то же самое дерево $b_n$ применяют к данной выборке с перемешанным признаком j\n",
    "и оценивают качество дерева на out-of-bag-подвыборке. $Q_{n}^{'}$ — ошибка out-of-bag-подвыборке, она будет тем\n",
    "больше, чем сильнее дерево использует признак j. Если он активно используется в дереве, то ошибка сильно\n",
    "уменьшится, поскольку значение данного признака испорчено. Если же данный признак совершенно не важен\n",
    "для дерева и не используется в нем, то ошибка практически не изменится. Таким образом, информативность\n",
    "признака j оценивают как разность\n",
    "$$Q_{n}^{'}−Q_n.$$\n",
    "Далее эти информативности усредняют по всем деревьям случайного леса, и чем больше будет среднее значе-\n",
    "ние, тем важнее признак. На практике оказывается, что информативности, вычисленные описанным образом,\n",
    "и информативности, вычисленные как сумма уменьшения критерия информативности, оказываются очень\n",
    "связаны между собой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Понижение размерности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаются новые признаки на основе начальных. Общее количество признаков уменьшается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
