{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_sites(path_to_csv_files):\n",
    "    \"\"\"Returns :\n",
    "    - dictionary of sites enumerated according frequency of use.\n",
    "    \"\"\"\n",
    "    # Vars:\n",
    "\n",
    "    # dictionary of site id's\n",
    "    sites = Counter()\n",
    "    # list of user files\n",
    "    fns = glob(path_to_csv_files+\"/*\")\n",
    "\n",
    "    # for every file\n",
    "    for f in fns:\n",
    "        # read user data\n",
    "        ud = pd.read_csv(os.path.join(PATH_TO_DATA, f))\n",
    "        # determine sites were visited\n",
    "        sites.update(Counter(ud['site']))\n",
    "        \n",
    "    # return dictionary with sorted and enumerated sites\n",
    "    result = {}\n",
    "    for i, x in enumerate(sites.most_common()):\n",
    "        result[x[0]] = (i+1, x[1])\n",
    "    return result, fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_matrix(docs):\n",
    "    \"\"\"\n",
    "    Returns parameters for construction sparse matrix from initial `docs` table.\n",
    "    vocabulary - dictionary of terms, value as index.\n",
    "    indicies - list of term indicies.\n",
    "    data - list of data existence\n",
    "    indptr - \n",
    "    \"\"\"\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for d in docs:\n",
    "        for term in d:\n",
    "                index = vocabulary.setdefault(term, term)\n",
    "                indices.append(index)\n",
    "                data.append(1)\n",
    "        indptr.append(len(indices))\n",
    "    return data, indices, indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sparse_train_set_window(path_to_csv_files, session_length=10, window_size=10):\n",
    "    \"\"\"Returns:\n",
    "    - train_data as sparse matrix;\n",
    "    - column of `user id` as answers\n",
    "    \"\"\"\n",
    "\n",
    "#     ----- Get sites (dictionary)\n",
    "    sites, fns = get_sites(path_to_csv_files)\n",
    "\n",
    "\n",
    "#     ----- Determine shape parameters of 'sparse matrix'\n",
    "\n",
    "#     Vars:\n",
    "#     sites usage data from .csv : (name , user_data, sessions_number)\n",
    "    ud = []\n",
    "#     for determination session number according input parameters: `session_length` and `window_size`\n",
    "    session_counter = 0\n",
    "\n",
    "#     For every .csv file\n",
    "    for f in fns:\n",
    "        #         Read user data (as DataFrame)\n",
    "        user_data = pd.read_csv(os.path.join(PATH_TO_DATA, f))\n",
    "        \n",
    "        #         Add 'sid' as 'site id' (info from sites)\n",
    "        user_data['site'] = user_data['site'].apply(lambda x: sites[x][0])\n",
    "        #         Determine number of sessions in the doc:\n",
    "        sessions_number = int(np.ceil(len(user_data)/window_size))\n",
    "        session_counter+= sessions_number\n",
    "        #\n",
    "        name = int(f[-8:-4])\n",
    "#         Save user data\n",
    "        ud.append((name , user_data, sessions_number))\n",
    "    \n",
    "#     ----- Construct training table (ndarray of zeros) with shape = (session_counter, session_length+1)\n",
    "    train_data = np.zeros(shape=(session_counter, session_length+1))\n",
    "\n",
    "\n",
    "#     ----- Fill training table\n",
    "\n",
    "#     Vars:\n",
    "#     Row counter\n",
    "    active_row = 0\n",
    "\n",
    "#     For every user data\n",
    "    for user_data in ud:\n",
    "        \"Read user data\"\n",
    "        name , doc, sessions_number = user_data\n",
    "\n",
    "#         Write session data (sites) to Training table:\n",
    "        for i in range(sessions_number):\n",
    "            start_row = i *window_size\n",
    "            end_row = start_row+session_length\n",
    "            for col, s in enumerate(doc['site'][start_row:end_row]):\n",
    "                train_data[active_row + i, col] = s\n",
    "            \n",
    "#         Write user data to train table\n",
    "        for i in range(active_row, active_row+sessions_number):\n",
    "            train_data[i, session_length] = name\n",
    "\n",
    "#         Update row counter\n",
    "        active_row += sessions_number\n",
    "    \n",
    "    \n",
    "#     ---- Divide training table on features and answers \n",
    "    X, y  = train_data[:, :-1], train_data[:, -1]\n",
    "    \n",
    "#     ---- Transform training table to sparse matrix with term features\n",
    "    X_sparse = csr_matrix(term_matrix(X))[:,1:]\n",
    "\n",
    "    return X_sparse, y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.4 ms, sys: 3.12 ms, total: 18.5 ms\n",
      "Wall time: 23.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[1, 3, 1, 0, 1, 0, 1, 1, 1, 1, 0],\n",
       "        [3, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [4, 2, 0, 2, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "path = \"3users\"\n",
    "train_data_toy, site_freq_3users = prepare_sparse_train_set_window(path)\n",
    "train_data_toy.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_toy_s5_w3, y_s5_w3 = prepare_sparse_train_set_window(os.path.join(PATH_TO_DATA,'3users'), \n",
    "                                       session_length=5, window_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 3, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\n",
       "        [3, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [2, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [1, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toy_s5_w3.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_s5_w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 : 10 : 15\n",
      "10 : 7 : 15\n",
      "10 : 7 : 10\n",
      "10 : 7 : 7\n",
      "10 : 5 : 15\n",
      "10 : 5 : 10\n",
      "10 : 5 : 7\n",
      "10 : 5 : 5\n",
      "150 : 10 : 15\n",
      "150 : 7 : 15\n",
      "150 : 7 : 10\n",
      "150 : 7 : 7\n",
      "150 : 5 : 15\n",
      "150 : 5 : 10\n",
      "150 : 5 : 7\n",
      "150 : 5 : 5\n",
      "CPU times: user 4min 33s, sys: 950 ms, total: 4min 34s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data_lengths = []\n",
    "\n",
    "for num_users in [10, 150]:\n",
    "    for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]):\n",
    "        if window_size <= session_length and (window_size, session_length) != (10, 10):\n",
    "            print(f\"{num_users} : {window_size} : {session_length}\")\n",
    "            X_sparse, y = prepare_sparse_train_set_window(os.path.join(PATH_TO_DATA, f'{num_users}users'),\n",
    "                                                          session_length, window_size)\n",
    "            data_lengths.append(len(y))\n",
    "            with open(os.path.join(PATH_TO_DATA, \n",
    "                                   f'X_sparse_{num_users}users_s{session_length}_w{window_size}.pkl'), \n",
    "                      'wb') as X_pkl:\n",
    "                pickle.dump(X_sparse, X_pkl, protocol=2)\n",
    "            with open(os.path.join(PATH_TO_DATA, \n",
    "                                   f'y_{num_users}users_s{session_length}_w{window_size}.pkl'), \n",
    "                      'wb') as y_pkl:\n",
    "                pickle.dump(y, y_pkl, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(data_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(i, answer):\n",
    "    with open(f\"task2_{i}_answer\", mode = \"w\") as file:\n",
    "        file.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \" \".join(map(str,data_lengths))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(1,answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_data_10users.csv'), \n",
    "                       index_col='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df.columns[10]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_sites = [np.unique(train_df.values[i, :-1]).shape[0] \n",
    "                    for i in range(train_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(num_unique_sites).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(num_unique_sites).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вопрос 2. Распределено ли нормально число уникальных сайтов в каждой сессии из 10 посещенных подряд сайтов (согласно критерию Шапиро-Уилка)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q\n",
    "%pylab inline\n",
    "from scipy.stats import probplot\n",
    "stats.probplot(num_unique_sites, dist=\"norm\", plot=pylab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Shapiro-Wilk test for normality.\n",
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pvalue = shapiro(num_unique_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test whether a sample differs from a normal distribution.\n",
    "from scipy.stats import normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normaltest(num_unique_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(2,\"NO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте гипотезу о том, что пользователь хотя бы раз зайдет на сайт, который он уже ранее посетил в сессии из 10 сайтов. Давайте проверим с помощью биномиального критерия для доли, что доля случаев, когда пользователь повторно посетил какой-то сайт (то есть число уникальных сайтов в сессии < 10) велика: больше 95% (обратите внимание, что альтернатива тому, что доля равна 95% – одностороняя). Ответом на 3 вопрос в тесте будет полученное p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_two_similar = (np.array(num_unique_sites) < 10).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_val = stats.binom_test(sum(has_two_similar), len(has_two_similar), p = 0.95, alternative=\"greater\")\n",
    "pi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(3,pi_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каков 95% доверительный интервал Уилсона для доли случаев, когда пользователь повторно посетил какой-то сайт (из п. 3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(has_two_similar)/ len(has_two_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wilson_interval = proportion_confint(sum(has_two_similar), len(has_two_similar), method = 'wilson')\n",
    "wilson_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} {}'.format(round(wilson_interval[0], 3),\n",
    "                                   round(wilson_interval[1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(4,'0.95 0.957')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"10users\"\n",
    "sparse10_X, sparse10_y = prepare_sparse_train_set_window(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse10_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = pd.Series(np.array(sparse10_X.sum(axis = 0))[0], name = 'freq')\n",
    "len(freq[freq>=1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 25 sites have more than 1000 visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_freqs =np.array(sparse10_X.sum(axis = 0))[0][:25]\n",
    "site_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.average(site_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поменяйте на свой путь к данным\n",
    "PATH_TO_DATA = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def get_sites(path_to_csv_files):\n",
    "    \"\"\"Returns :\n",
    "    - dictionary of sites were used by users.\n",
    "    \"\"\"\n",
    "    # Vars:\n",
    "\n",
    "    # dictionary of site id's\n",
    "    sites = Counter()\n",
    "    # list of user files\n",
    "    fns = glob(path_to_csv_files+\"/*\")\n",
    "\n",
    "    # for every file\n",
    "    for f in fns:\n",
    "        # read user data\n",
    "        ud = pd.read_csv(os.path.join(PATH_TO_DATA, f))\n",
    "        # determine sites were visited\n",
    "        sites.update(Counter(ud['site']))\n",
    "        \n",
    "    # return dictionary with sorted and enumerated sites\n",
    "    result = {}\n",
    "    for i, x in enumerate(sites.most_common()):\n",
    "        result[x[0]] = (i+1, x[1])\n",
    "    return result, fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '10users'\n",
    "sites = get_sites(path)\n",
    "s = sites[0]\n",
    "\n",
    "s = [(v[0], v[1]) for k, v in s.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4913"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(s);n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8300, 7813, 5441, ...,    1,    1,    1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([x[1] for x in s])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_samples(data, n_samples, random_seed=17):\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    samples = data[indices]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_scores = list(map(np.average, get_bootstrap_samples(data, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.618690556617366"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(avg_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stat_intervals(stat, alpha):\n",
    "    boundaries = np.percentile(stat, \n",
    "                 [100 * alpha / 2., 100 * (1 - alpha / 2.)])\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = stat_intervals(avg_scores, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['22.515', '35.763']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = lambda x : str(round(x,3))\n",
    "list(map(r, interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22.515 35.763'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = \" \".join(list(map(r, interval)))\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answer(i, answer):\n",
    "    with open(f\"task2_{i}_answer\", mode = \"w\") as file:\n",
    "        file.write(str(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_answer(5, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
